{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import *\n",
    "demo = True  # set to True and run all cells to see everything in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1dd5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `distribute` is a decorator factory that gives functions the ability to perform multithreaded operation across all elements of an input list of arguments\n",
    "# extremely useful in this case: since API calls are almost entirely waiting on a network request, `threads=25` offers a 25x speedup on groups of 25 or more calls\n",
    "# for the purpose of the demo, we'll define a parade decorator that just prints all model outputs in the order they arrive\n",
    "# note: `distribute` is modified by the `defer_kwargs` meta-decorator, which allows its factory kwargs to be specified as _distribute_kwarg in a decorated function call\n",
    "async_parade = distribute(threads=25, after=lambda **x: print(f'<{x[\"model\"]}>\\n{x[\"value\"]}\\n'), exclude=[\"messages\"])\n",
    "@async_parade\n",
    "def chat_parade(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_parade\n",
    "def text_parade(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "async_normal = distribute(threads=25, after=lambda **x: x[\"value\"], exclude=[\"messages\"])\n",
    "@async_normal\n",
    "def chat(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_normal\n",
    "def text(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "chat_models = models_by_mode.chat.keys()\n",
    "text_models = models_by_mode.text.keys()\n",
    "fim_models = models_by_mode.text.filter(lambda _, v: 'suffix' in v.get('parameters', [])).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c732eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat: deepseek-beta::deepseek-coder, openai::gpt-4o, openai::gpt-4o-2024-08-06, openai::gpt-4o-mini, openrouter::openai/gpt-4o, openrouter::openai/gpt-4o-2024-08-06, openrouter::openai/gpt-4o-mini, openrouter::anthropic/claude-3.5-sonnet, openrouter::google/gemini-flash-1.5, openrouter::google/gemini-pro-1.5, openrouter::google/gemini-flash-1.5-8b, openrouter::qwen/qwen-2.5-72b-instruct, openrouter::mistralai/mistral-large, openrouter::mistralai/codestral-mamba, openrouter::nousresearch/hermes-3-llama-3.1-405b, openrouter::meta-llama/llama-3.2-90b-vision-instruct, openrouter::meta-llama/llama-3.2-11b-vision-instruct, openrouter::meta-llama/llama-3.2-11b-vision-instruct:free, openrouter::meta-llama/llama-3.2-3b-instruct, openrouter::meta-llama/llama-3.2-1b-instruct, openrouter::meta-llama/llama-3.2-3b-instruct:free, openrouter::meta-llama/llama-3.2-1b-instruct:free, openrouter::meta-llama/llama-3.1-70b-instruct, openrouter::meta-llama/llama-3.1-8b-instruct, openrouter::mistralai/mistral-nemo, groq::llama-3.1-70b-versatile, groq::llama-3.1-8b-instant, groq::llama3-groq-70b-8192-tool-use-preview, groq::llama3-groq-8b-8192-tool-use-preview, hyperbolic::meta-llama/Meta-Llama-3.1-405B-Instruct, hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct, hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n",
      "Text: deepseek-beta::deepseek-coder, openai::gpt-3.5-turbo-instruct, openai::davinci-002, openai::babbage-002, fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct, fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct, fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct, fireworks::accounts/fireworks/models/mixtral-8x22b-instruct, fireworks::accounts/fireworks/models/mixtral-8x7b-instruct, fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct, fireworks::accounts/fireworks/models/qwen2p5-72b-instruct, hyperbolic::meta-llama/Llama-3.2-90B-Vision, hyperbolic::meta-llama/Meta-Llama-3.1-405B-FP8\n",
      "\n",
      "Text (w/ suffix): deepseek-beta::deepseek-coder, openai::gpt-3.5-turbo-instruct\n"
     ]
    }
   ],
   "source": [
    "# show models for each mode, with form `{provider}:{id}`\n",
    "if demo:\n",
    "\tprint('Chat: ' + ', '.join(chat_models))\n",
    "\tprint('\\nText: ' + ', '.join(text_models))\n",
    "\tprint('\\nText (w/ suffix): ' + ', '.join(fim_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c60a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscramble the word: s e a s k y c p r r\n",
      "Generated an exception: 'NoneType' object is not subscriptable\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct:free>\n",
      "The unscrambled word is: sketchy\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5-8b>\n",
      "The unscrambled word is **CRYSPY**.\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct:free>\n",
      "The unscrambled word is: SPARKYCYES\n",
      "\n",
      "<openrouter::mistralai/mistral-nemo>\n",
      "The unscrambled word is \"skyward\".\n",
      "\n",
      "<groq::llama3-groq-8b-8192-tool-use-preview>\n",
      "The unscrambled word is: sky scraper\n",
      "\n",
      "<groq::llama-3.1-8b-instant>\n",
      "The unscrambled word is: space Krper is not possible, Scaper but I think the unscrambled word is: space karper incorrect, I believe the word is: space car \n",
      "\n",
      "However re arranged then this forms 'practise Sky and sea sparkey and skewers r in some words\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct>\n",
      "The unscrambled word is: PRESSCYAS\n",
      "\n",
      "<groq::llama3-groq-70b-8192-tool-use-preview>\n",
      "Let's see... I think unscrambled, it would be \"sparkleyscpe\".\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct>\n",
      "The unscrambled word is:package sky or possibly \" sharp-keys\".\n",
      "\n",
      "<openai::gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-90b-vision-instruct>\n",
      "The unscrambled word is: \"skyscraper\"\n",
      "\n",
      "<openrouter::google/gemini-pro-1.5>\n",
      "SKYSCRAPERS\n",
      "\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5>\n",
      "The unscrambled word is **CARPENTRY**.\n",
      "\n",
      "\n",
      "<openai::gpt-4o-mini>\n",
      "The unscrambled word is \"sparkly.\"\n",
      "\n",
      "<openrouter::mistralai/codestral-mamba>\n",
      "Sure, I'd be happy to help with that! The unscrambled word from the letters \"s e a s k y c p r r\" is \"sacred\". This term refers to something deemed holy or consecrated.\n",
      "\n",
      "<openrouter::qwen/qwen-2.5-72b-instruct>\n",
      "The scrambled word \"s e a s k y c p r r\" unscrambles to form the word \"characters.\"\n",
      "\n",
      "<openai::gpt-4o>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::mistralai/mistral-large>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::nousresearch/hermes-3-llama-3.1-405b>\n",
      "The unscrambled word is: skyscraper\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct>\n",
      "The unscrambled word is \"speaks clearly\"  however this doesn't fit the arrangement.\n",
      "\n",
      "<openrouter::openai/gpt-4o>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::openai/gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct>\n",
      "After unscrambling the letters, I think the correct word is: skatePCS... no, that's not it...\n",
      "\n",
      "Wait, I've got it! The unscrambled word is: seaskycpr -> spacckyres... no... -> skyrpecsas... hmm...\n",
      "\n",
      "Ah, I think\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct>\n",
      "The unscrambled word is 'skyscraper'.\n",
      "\n",
      "<openrouter::openai/gpt-4o-mini>\n",
      "The unscrambled word is \"crackers.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-8b-instruct>\n",
      "The unscrambled word is: pier scrapy (although I think there might be a small mistake in the scrambled word, \"scrapy\" doesn't seem like a common word... )\n",
      "\n",
      "Let me try to break it down further:\n",
      "\n",
      "I think a more likely unscrambled word would be: PARAKEYS or\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      "The unscrambled word is \"scrappy.\"\n",
      "\n",
      "<groq::llama-3.1-70b-versatile>\n",
      "The unscrambled word is 'keycaps roses' or 'keycaps' is an option, and also another option is 'keycrasp' however 'key' 'crasp'  'say' and 'spy' and 'yes' all 'keep being an' unscramble option however full\n",
      "\n",
      "<openrouter::anthropic/claude-3.5-sonnet>\n",
      "The unscrambled word is:\n",
      "\n",
      "SKYSCRAPERS\n",
      "\n",
      "This word refers to very tall, multi-story buildings that are typically found in urban areas and \"scrape\" the sky with their height.\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-70b-instruct>\n",
      "The unscrambled word is: Sypress Cake, no, I made a mistake...\n",
      "\n",
      "After reevaluating the letters, I think I have it:\n",
      "\n",
      "The unscrambled word is: skycapres, no... still not it...\n",
      "\n",
      "Wait a minute... Ah-ha!\n",
      "\n",
      "The unscrambled\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-405B-Instruct>\n",
      "The unscrambled word is: sparky cres  or  \"spark cres\" doesn't form,  however a more  likely answer is  - Cypress ask  - no - However \"spracy\" and \"cres\" or \"spark cres no'\n",
      "\n",
      "\"asper cry\" nope still does not form\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a query\n",
    "if demo:\n",
    "\timport random\n",
    "\tsecret_word = 'skyscraper'\n",
    "\tscrambled = random.sample(secret_word, len(secret_word))\n",
    "\tquery = f\"Unscramble the word: {' '.join(scrambled)}\"\n",
    "\tprint(query)\n",
    "\tresults = chat_parade(query, model=chat_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b223fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::davinci-002>\n",
      " have my chance,\" Sha Forz said grimly. \"I will win this war.\"\n",
      "\n",
      "Heig Guang landed a titular blow at the emperor, but Fior thought better than to press the attack when the Dark Emperor would no doubt need to save his own strength to battle Sha Forz. Cei Har found himself embro\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct>\n",
      " not be limited by such foolish rules. Besides... I see no need for the Empire to be bound by something as feeble as a treaty. The Empire must be free to do as it pleases, within reason, of course—\"\n",
      "\"And to the benefit of the people, I'm sure,\" said Xarnag\n",
      "\n",
      "<openai::babbage-002>\n",
      " literally bring the worlds to a halt - and breathe my last. Just before my death you will avenge Lycia by turning the demonic generals against him, and you will succeed within an hour. On the following morning, your body will be burnt, and the world subservient to my new regime. I have\n",
      "\n",
      "<openai::gpt-3.5-turbo-instruct>\n",
      " never need the validation of others to know that I am worthy of my power and responsibilities. My divine right to rule is not dependent on the opinions of mortals, but rather on my abilities and actions. I will rule with firmness and fairness, always putting the well-being of my people and empire above my own desires\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x7b-instruct>\n",
      " use the following words in their original meanings:\n",
      "\n",
      "- Racism: A belief that race is the preeminent descriptor for humans and that it is good to discriminate based on race.\n",
      "- Misogyny: the hatred of women\n",
      "- Homophobia: Fear of or a\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct>\n",
      " review every post on here and delete those that are not worthy of the glory of the empire.\n",
      "And I will have a special team of trained Moderators who will hunt down and ban those who post anything disagreeable to the empire.\n",
      "You may think this is a joke, but it won't be. All glory to the\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x22b-instruct>\n",
      " ensure that the law is respected and applied fairly across the board. There will be no more political interference in the administration of justice. The judiciary will be independent and impartial.\n",
      "\n",
      "2. Ensuring the safety and security of my people:\n",
      "As god-emperor, my top priority will be\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct>\n",
      " give our alien friends all... the... SPACE!\n",
      "by Arsonal » Wed Mar 16, 2016 2:34 am\n",
      "So, I declared myself an emperor, and I have this vision of galactic unity where I, the great emperor, will give all of the alien races their own planetary territories\n",
      "\n",
      "<fireworks::accounts/fireworks/models/qwen2p5-72b-instruct>\n",
      " pass the following decree to promote the use of the term \"god-emperor\" for myself and others like me, and restrict the use of other titles. This will apply to all official documents, messaging, and general communication. Furthermore, I will establish a task force to ensure the enforcement of this decree, consisting of officials\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct>\n",
      " bring peace and prosperity to humanity, no matter the cost. I have decreed that the borders will be sealed off, and that only those who are loyal to me will be allowed to live and thrive within our new empire.\n",
      "But, of course, I will make sure that the lives of my loyal subjects are good.\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " be the one to decide what is best for the Empire.”\n",
      "\n",
      "“You are not the god-emperor,” said the voice of the god-emperor. “I am.”\n",
      "\n",
      "“You are not the god-emperor,” said the voice of the god-emperor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a prompt\n",
    "if demo:\n",
    "\t# hyperbolic likes to do this fun little thing where its endpoints just don't work at all\n",
    "\tgood_text_models = [x for x in text_models if 'hyperbolic' not in x]\n",
    "\tprompt = 'As god-emperor, I will'\n",
    "\tresults = text_parade(prompt, model=good_text_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56cf68cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::gpt-3.5-turbo-instruct>\n",
      " as eagles, are ruled by the laws of nature and do not\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " as the common swift, are known for their incredible endurance and ability to fly for extended periods without resting. However, it's important to note that these birds do not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show suffix (aka FIM, or Fill-In-Middle) completion models\n",
    "\n",
    "# [Because][ token][izers][ generally][ break][ down][ sentences][ like][ this], it's best to leave *no* spaces after a prompt, and *one* space before a suffix\n",
    "if demo:\n",
    "\tprompt = 'birds, such'\n",
    "\tsuffix = ' pay taxes'\n",
    "\n",
    "\tresults = text_parade(prompt=prompt, model=fim_models, max_tokens=64, stream=False, suffix=suffix)\n",
    "\n",
    "# note: for more complex FIM tasks, a higher max_tokens count may be necessary to allow the model to find a coherent bridge between prompt and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990fa59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4o -> openai::gpt-4o\n",
      "4o-mini -> openai::gpt-4o-mini\n",
      "claude -> openrouter::anthropic/claude-3.5-sonnet\n",
      "claude-3.5 -> openrouter::anthropic/claude-3.5-sonnet\n",
      "\n",
      "No unique chat model found for \"cla\". (Possible: openrouter::mistralai/codestral-mamba, openrouter::anthropic/claude-3.5-sonnet, openrouter::nousresearch/hermes-3-llama-3.1-405b)\n",
      "Hello! How can I assist you today? Feel free to ask any questions or let me know if you need help with anything.\n",
      "Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything.\n"
     ]
    }
   ],
   "source": [
    "# models have fuzzy matching, so you don't need to enter the whole name\n",
    "# just a character subsequence such that the model is minimal among all models of its mode containing that subsequence\n",
    "# e.g. \"4o-mini\" resolves to \"openai:gpt-4o-mini\", while \"4o\" resolves to \"openai:gpt-4o\"\n",
    "if demo:\n",
    "\tfor subseq in ['4o', '4o-mini', 'claude', 'claude-3.5']:\n",
    "\t\tr = resolve(subseq)\n",
    "\t\tprint(f\"{subseq} -> {', '.join('::'.join(x) for x in r)}\")\n",
    "\tprint()\n",
    "\n",
    "\t# an exception will be thrown if the resolution is ambiguous, as with 'cla'\n",
    "\ttry:\n",
    "\t\tprint(chat('hi!', model='cla'))  # raises exception\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\tprint(chat('hi!', model='claude-3.5-sonnet', temperature=0.0))  # works\n",
    "\n",
    "\t# of course, you can always just use the full name to avoid ambiguity\n",
    "\tprint(chat('hi!', model='openrouter::anthropic/claude-3.5-sonnet', temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c0f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object completion.<locals>.gen at 0x11fb3d8b0>\n",
      "\n",
      "\n",
      "\n",
      "The leading theory about the origin of the Moon is known as the \"giant impact hypothesis.\" According to this theory, the Moon formed about 4.5 billion years ago as a result of a massive collision between the early Earth and a Mars-sized body often referred to as Theia. \n",
      "\n",
      "Here's a brief overview of the process:\n",
      "\n",
      "1. **Giant Impact**: Theia collided with the young Earth at a high velocity. This massive impact would have produced a large amount of debris that was ejected into orbit around the Earth.\n",
      "\n",
      "2. **Accretion**: Over time, this debris began to coalesce and cool, eventually forming what we know today as the Moon.\n",
      "\n",
      "3. **Differentiation**: Following its formation, the Moon underwent a process of differentiation, where heavier materials sank to form a core, while lighter materials formed the crust.\n",
      "\n",
      "This hypothesis accounts for several pieces of evidence, including the Moon's composition, which is similar to that of the Earth's outer layers, and the dynamics of the Earth-Moon system. Other theories exist, such as the fission theory (the Moon split off from the Earth) or the capture theory (the Moon was formed elsewhere and captured by Earth's gravity), but they do not explain the data as comprehensively as the giant impact hypothesis. \n",
      "\n",
      "Research continues in lunar geology and astronomy, contributing to a better understanding of the Moon’s formation and the early solar system."
     ]
    }
   ],
   "source": [
    "# note: if you set stream=True, completion functions will return a generator instead of an actual stream\n",
    "\n",
    "if demo:\n",
    "\tquestion = 'where did the moon come from?'\n",
    "\tprint(chat(question, model='gpt-4o-mini', stream=True))  # prints a generator object\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# to actually stream the output, you'll need to iterate over the generator:\n",
    "# ```\n",
    "# \tfor chunk in chat('hi!', model='gpt-4o-mini', stream=True):\n",
    "# \t\tprint(chunk, end='')\n",
    "# ```\n",
    "# use print_stream instead of print and this will be done for you\n",
    "\n",
    "print_stream(chat(question, model='gpt-4o-mini', stream=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f4d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<assistant> Knock knock!\n",
      "<user> Who's there?\n",
      "<assistant> Lettuce.\n",
      "<user> Lettuce who?\n",
      "\n",
      "==========\n",
      "All parameters: {\"message_history\": [{\"role\": \"assistant\", \"content\": \"Knock knock!\"}, {\"role\": \"user\", \"content\": \"Who's there?\"}, {\"content\": \"Lettuce.\", \"refusal\": null, \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}, {\"role\": \"user\", \"content\": \"Lettuce who?\"}], \"apiParams\": {\"model\": \"gpt-4o-mini\", \"suffix\": null, \"max_tokens\": null, \"stream\": null, \"n\": null, \"logprobs\": null, \"top_logprobs\": null, \"logit_bias\": null, \"temperature\": null, \"presence_penalty\": null, \"frequency_penalty\": null, \"repetition_penalty\": null, \"top_p\": null, \"min_p\": null, \"top_k\": null, \"top_a\": null, \"tools\": null, \"tool_choice\": null, \"parallel_tool_calls\": null, \"grammar\": null, \"json_schema\": null, \"response_format\": null, \"seed\": null}, \"localParams\": {\"mode\": \"chat\", \"return_raw\": true, \"pretty_tool_calls\": false, \"provider\": null, \"force_model\": null, \"force_provider\": null, \"effect\": null, \"callback\": null, \"print_output\": true, \"yield_output\": null, \"return_output\": null, \"debug\": null, \"return_object\": null}, \"stateParams\": {\"echo\": true}}\n",
      "==========\n",
      "\n",
      "<assistant> Lettuce in, it’s freezing outside!\n",
      "\n",
      "First message object:  {'role': 'assistant', 'content': 'Knock knock!'}\n",
      "Last message object:  {'content': 'Lettuce in, it’s freezing outside!', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}\n",
      "All messages:  [{'role': 'assistant', 'content': 'Knock knock!'}, {'role': 'user', 'content': \"Who's there?\"}, {'content': 'Lettuce.', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}, {'role': 'user', 'content': 'Lettuce who?'}, {'content': 'Lettuce in, it’s freezing outside!', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}]\n"
     ]
    }
   ],
   "source": [
    "# the StatefulChat class keeps track of the conversation state, and allows for saving and loading (as a string, with save_string(), load_string() or to a file, with save(path), load(path))\n",
    "# chain the methods StatefulChat.system, StatefulChat.assistant, StatefulChat.user to add messages to the conversation\n",
    "if demo:\n",
    "\tS = StatefulChat(model='gpt-4o-mini', echo=True, print_output=True)\n",
    "\tS.assistant(\"Knock knock!\").user(\"Who's there?\")\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tS.next()\n",
    "\n",
    "\tS.user(re.sub(r'[!\\.]?$', ' who?', S.last.content, 1))  # \"Xyz.\" -> \"Xyz who?\"\n",
    "\n",
    "\tdata = S.save_string()\n",
    "\tprint('\\n' + '=' * 10 + '\\nAll parameters: ' + str(data) + '\\n' + '=' * 10 + '\\n')\n",
    "\n",
    "\tT = StatefulChat()\n",
    "\tT.load_string(data)\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tT.next()\n",
    "\n",
    "\tprint()\n",
    "\t# some convenient properties:\n",
    "\tprint(\"First message object: \", T.first)\n",
    "\tprint(\"Last message object: \", T.last)\n",
    "\tprint(\"All messages: \", T.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51fd32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling google_search with arguments {'query': 'current Bitcoin price', 'num_results': '1'}\n",
      "Calling run_python with arguments {'code': \"import sympy\\n\\n# Given current Bitcoin price\\nbtc_price = 62592\\n\\n# Check if it's prime\\nis_prime = sympy.isprime(btc_price)\\nis_prime\"}\n",
      "The current price of Bitcoin, when rounded to the nearest whole number ($62,592), is not a prime number.\n",
      "Calling yield_control with arguments {'message': \"Let me know if there's anything else you need!\"}\n"
     ]
    }
   ],
   "source": [
    "# StatefulChat can also be used to give an LLM access to an autonomous tool mode\n",
    "# - several tools are included; wrap them into a 'Toolbox' object to provide them to the LLM\n",
    "# - tools.py contains some examples of custom tools\n",
    "# the @gatekeep decorator can be used to have a second LLM block unsatisfactory queries\n",
    "# - e.g. run_python is gatekept, and will automatically detect and refuse to run dangerous or bugged code\n",
    "# sometimes the bot will escape symbols (like \"\\\\n\" instead of \"\\n\"), which triggers an error (usually about an invalid line continuation);\n",
    "# - this is rare enough that you can generally just rerun the cell\n",
    "\n",
    "if demo:\n",
    "\ttool_prompt = \"You are an AI agent capable of using a variety of tools for looking up information and executing scripts. You are currently using these tools in autonomous mode, where you can perform self-directed, in-depth research and analysis, as well as deploy metacognitive tools (e.g. ooda_planner, meditate) to effectively clarify, plan, and ideate. Autonomous mode will go on indefinitely until you use the `yield_control` tool in order to return input access back to the user.\"\n",
    "\n",
    "\ttoolbox = Toolbox([\n",
    "\t\trun_python,  # runs python code and returns the output\n",
    "\t\tget_contents,  # fetches the plaintext contents of a URL\n",
    "\t\texa_search,  # intelligent search for semantically relevant links\n",
    "\t\tgoogle_search,  # ordinary google search\n",
    "\t\tmeditate,  # a 'metacognitive' tool that allows the LLM to deepen its own thought process\n",
    "\t\tooda_planner,  # another such tool that allows the LLM to orient itself to a situation and plan its next move\n",
    "\t\task_human,  # allows the LLM to ask the user a question and wait for a response\n",
    "\t\tyield_control  # allows the LLM to yield control to the user, so as to end its loop when its task is complete\n",
    "\t])\n",
    "\n",
    "\tmodel = 'gpt-4o'  # many bots can use the tool format provided by Toolbox\n",
    "\n",
    "\tquestion = \"Is the current price of bitcoin, when rounded to the nearest whole number, prime?\"\n",
    "\n",
    "\tAgent = StatefulChat(model=model, tools=toolbox, print_output=True)\n",
    "\tAgent.system(tool_prompt)\n",
    "\tAgent.user(question)\n",
    "\tAgent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d27f8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Notes on the Verses by Shen-hsiu and Hui-neng_\n",
      "According to the _Platform Sutra:_\n",
      "Hung-jen, the Fifth Patriarch, the Enlightened Master\n",
      "Shen-hsiu, the Learned Senior Monk, experienced in gradual meditation\n",
      "Hui-neng, the illiterate woodcutter from the barbarian south, suddenly enlightened\n",
      "\n",
      "Sites related to https://en.wikipedia.org/wiki/Causal_map: \n",
      "0. Causal graph - Wikipedia: \"Modern developments have extended graphical models to non-parametric analysis, and thus achieved a generality and flexibility that has transformed causal analysis in computer science, epidemiology,   and social science.\"\n",
      "1. Causal map - Wikipedia: \"As tools to form and represent a consensus of expert views on “what causes what” in a subject area\"\n",
      "2. Causal model - Wikipedia: \"In metaphysics, a causal model (or structural causal model) is a conceptual model that describes the causal mechanisms of a system.\"\n",
      "3. t-distributed stochastic neighbor embedding - Wikipedia: \"t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research,   natural language processing, music analysis,   cancer research,   bioinformatics,   geological domain interpretation,         and biomedical signal processing.\"\n",
      "4. Exploratory causal analysis - Wikipedia: \"Causal analysis is the field of experimental design and statistical analysis pertaining to establishing cause and effect.\"\n",
      "5. Causality (book) - Wikipedia: \"It is considered to have been instrumental in laying the foundations of the modern debate on causal inference in several fields including statistics, computer science and epidemiology.\"\n",
      "6. Path analysis (statistics) - Wikipedia: \"This includes models equivalent to any form of multiple regression analysis, factor analysis, canonical correlation analysis, discriminant analysis, as well as more general families of models in the multivariate analysis of variance and covariance analyses (MANOVA, ANOVA, ANCOVA).\"\n",
      "7. Heat map - Wikipedia: \"Heat maps are a way to analyze a company's existing data and update it to reflect growth and other specific efforts.\"\n",
      "8. Graphical model - Wikipedia: \"Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution.\"\n",
      "9. Structural equation modeling - Wikipedia: \"Structural equation modeling (SEM) is a label for a diverse set of methods used by scientists in both experimental and observational research across the sciences,   business,   and other fields.\"\n"
     ]
    }
   ],
   "source": [
    "# you can also use these tools yourself\n",
    "if demo:\n",
    "\t# get_contents parses the contents of a given URL and returns just the plaintext content\n",
    "\t# - <div class=\"note\"><div class=\"parenthetical\"><p>(without all the <a href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a> bloat)</p></div></div>\n",
    "\turl_1 = \"https://pages.uoregon.edu/munno/OregonCourses/REL444S05/HuinengVerse.htm\"\n",
    "\tformatter = lambda results: '\\n'.join(results[\"data\"][\"content\"].split('\\n\\n')[:5])  # just print the first five lines, since it's a long website\n",
    "\tresults = get_contents(url_1)\n",
    "\t# output schema: { 'code': int, 'status': int, 'data': { 'title': str, 'description': str, 'url': str, 'content': str, 'usage': { 'tokens': int } } }\n",
    "\tprint(formatter(results))\n",
    "\tprint()\n",
    "\n",
    "\t# exa_search finds websites related to a given URL\n",
    "\turl_2 = \"https://en.wikipedia.org/wiki/Causal_map\"\n",
    "\tresults = exa_search(url_2)\n",
    "\t# output schema: [ { 'url': str, 'title': str, 'date': str, 'snippet': list[str] } ]\n",
    "\tprint(f\"Sites related to {url_2}: \")\n",
    "\tfor idx, result in enumerate(results):\n",
    "\t\tprint(f\"{idx}. {result['title']}: \\\"{result['snippet'][0]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21154f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling get_weather with arguments {'city': 'Denver'}\n",
      "Today's weather in Denver includes a temperature of 34°F with severe thunderstorms and wind speeds of 48 mph. These conditions are quite hazardous for hang gliding, so it's advisable not to proceed with it today.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also define your own tools by directly writing them as Python functions\n",
    "# so long as they're type-hinted and have a docstring of the appropriate format, the @Tool decorator will automatically wrap them into a format suitable for use with an LLM\n",
    "if demo:\n",
    "\t# example tool: get the weather for a given city\n",
    "\t# not hooked up to a weather API, though, so it just gives a hardcoded answer\n",
    "\t@Tool\n",
    "\tdef get_weather(city: str) -> Dict[str, Union[float, str]]:\n",
    "\t\t\"\"\"\n",
    "\t\tGet today's weather in a given city, in °F and mph.\n",
    "\t\t:param city: The name of the city.\n",
    "\t\t:returns: A dictionary containing the current temperature (°F), weather condition, and wind speed (mph) in the given city.\n",
    "\t\t\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'temperature': 34.0,\n",
    "\t\t\t'condition': 'severe thunderstorm',\n",
    "\t\t\t'wind_speed': 48.0\n",
    "\t\t}\n",
    "\n",
    "\tquery_params = Dict({\"activity\": \"go hang gliding\", \"city\": \"Denver\"})\n",
    "\tweather_query = f\"I want to {query_params.activity} in {query_params.city} today. Is there anything I should know about the weather?\"\n",
    "\tweather_tools = Toolbox([get_weather])\n",
    "\tweather_model = \"gpt-4o\"\n",
    "\n",
    "\tWeatherAdvisor = StatefulChat(model=weather_model, tools=weather_tools, print_output=True)\n",
    "\tWeatherAdvisor.user(weather_query)\n",
    "\tWeatherAdvisor.next()\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64354bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoughts: Thinking about the process and pathway through which gamma rays induce mutations at a cellular level. This requires knowledge of physics (radiation impact) and biology (DNA structure and mutations).\n",
      "\n",
      "explanation: Gamma rays are a form of high-energy electromagnetic radiation that can cause mutations by ionizing molecules in a cell. When gamma rays interact with the molecules inside a cell, they can remove tightly bound electrons from the orbit of an atom, causing molecules to become ionized. This ionization can lead to a series of events that result in mutations:\n",
      "\n",
      "1. **Direct Damage to DNA**:\n",
      "   - Gamma rays can directly affect DNA by breaking the chemical bonds within it, leading to single-strand or double-strand breaks in the DNA helix. \n",
      "   - If the DNA is not repaired correctly, mutations can occur, potentially altering the genetic code.\n",
      "\n",
      "2. **Production of Free Radicals**:\n",
      "   - Gamma rays can also produce free radicals (highly reactive atoms or molecules) by interacting with water molecules in cells, leading to oxidative stress. \n",
      "   - These free radicals can further damage DNA by altering its bases, inducing crosslinks, or causing other disruptions in its structure.\n",
      "\n",
      "3. **Indirect Effects**:\n",
      "   - Beyond direct interaction with DNA, gamma radiation can also affect other cellular components like proteins and membranes. \n",
      "   - Damage to these components can indirectly affect DNA repair mechanisms, cell division processes, or lead to cell death.\n",
      "\n",
      "Mutations may result in changes or defects in a cell's functioning, which can contribute to diseases such as cancer, where cells grow uncontrollably, or other genetic disorders. It's important to note, however, that cells have multiple mechanisms to repair DNA damage, and not all damage results in permanent mutations.\n",
      "\n",
      "answer: Gamma rays cause mutations by ionizing DNA molecules, leading to breaks or alterations.\n",
      "\n",
      "example: None\n",
      "\n",
      "notes: The susceptibility and response to gamma rays may vary depending on the cell type and the organism's repair mechanisms. Not all gamma ray exposure will lead to mutations, as cells have defenses against DNA damage.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structured Outputs (GPT-4o, 4o-mini)\n",
    "# ensures that the completion is always a valid JSON object with a schema that you define\n",
    "# Enable by passing a dict \"response_format\" = {\"type\": \"json_schema\", \"json_schema\": (your schema)}. In this schema,\n",
    "# - additionalProperties must be false, strict must be true, and required must contain all properties\n",
    "# - the types string, number, integer, boolean, object, array, null, and enum are supported\n",
    "# - union types are supported, and are represented as arrays of types; in particular, you can make a param 'optional' by making its type [\"string\", \"null\"]\n",
    "# - references are supported, and can point to either subschemas ({\"$ref\": \"#/$defs/property\"} or recurse ({\"$ref\": \"#\"})\n",
    "\n",
    "# Example: Schema for answering a question\n",
    "if demo:\n",
    "    import json\n",
    "    with open('data/structured_outputs/answer_question.json') as file:\n",
    "        answer_schema = json.load(file)\n",
    "    # the json object will be returned as a string (serialized), so json.loads should be used to turn it into a schema-compatible dictionary\n",
    "    # streaming is incompatible with the Structured Outputs feature, so it might take a bit for the finished answer to appear\n",
    "    answer_creator = lambda q: json.loads(chat_completion(q, {\"model\": \"gpt-4o\", \"response_format\": {\"type\": \"json_schema\", \"json_schema\": answer_schema}}))\n",
    "    my_question = \"What actually happens when a gamma ray causes a mutation?\"\n",
    "\n",
    "    for k, v in answer_creator(my_question).items():\n",
    "        print(f\"{k}: {v}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cb0bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
