{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c8d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.piping import *\n",
    "demo = True  # set to True and run all cells to see everything in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dd5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `distribute` is a decorator factory that gives functions the ability to perform multithreaded operation across all elements of an input list of arguments\n",
    "# extremely useful in this case: since API calls are almost entirely waiting on a network request, `threads=25` offers a 25x speedup on groups of 25 or more calls\n",
    "# for the purpose of the demo, we'll define a parade decorator that just prints all model outputs in the order they arrive\n",
    "# note: `distribute` is modified by the `defer_kwargs` meta-decorator, which allows its factory kwargs to be specified as _distribute_kwarg in a decorated function call\n",
    "async_parade = distribute(threads=25, after=lambda **x: print(f'<{x[\"model\"]}>\\n{x[\"value\"]}\\n'), exclude=[\"messages\"])\n",
    "@async_parade\n",
    "def chat_parade(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_parade\n",
    "def text_parade(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "async_normal = distribute(threads=25, after=lambda **x: x[\"value\"], exclude=[\"messages\"])\n",
    "@async_normal\n",
    "def chat(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_normal\n",
    "def text(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "chat_models = models_by_mode.chat.keys()\n",
    "text_models = models_by_mode.text.keys()\n",
    "fim_models = models_by_mode.text.filter(lambda _, v: 'suffix' in v.get('parameters', [])).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c732eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat: deepseek-beta::deepseek-coder, openai::gpt-4o, openai::gpt-4o-2024-08-06, openai::gpt-4o-mini, openrouter::openai/gpt-4o, openrouter::openai/gpt-4o-2024-08-06, openrouter::openai/gpt-4o-mini, openrouter::anthropic/claude-3.5-sonnet, openrouter::google/gemini-flash-1.5, openrouter::google/gemini-pro-1.5, openrouter::google/gemini-flash-1.5-8b, openrouter::qwen/qwen-2.5-72b-instruct, openrouter::mistralai/mistral-large, openrouter::mistralai/codestral-mamba, openrouter::nousresearch/hermes-3-llama-3.1-405b, openrouter::meta-llama/llama-3.2-90b-vision-instruct, openrouter::meta-llama/llama-3.2-11b-vision-instruct, openrouter::meta-llama/llama-3.2-11b-vision-instruct:free, openrouter::meta-llama/llama-3.2-3b-instruct, openrouter::meta-llama/llama-3.2-1b-instruct, openrouter::meta-llama/llama-3.2-3b-instruct:free, openrouter::meta-llama/llama-3.2-1b-instruct:free, openrouter::meta-llama/llama-3.1-70b-instruct, openrouter::meta-llama/llama-3.1-8b-instruct, openrouter::mistralai/mistral-nemo, groq::llama-3.1-70b-versatile, groq::llama-3.1-8b-instant, groq::llama3-groq-70b-8192-tool-use-preview, groq::llama3-groq-8b-8192-tool-use-preview, hyperbolic::meta-llama/Meta-Llama-3.1-405B-Instruct, hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct, hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n",
      "Text: deepseek-beta::deepseek-coder, openai::gpt-3.5-turbo-instruct, openai::davinci-002, openai::babbage-002, fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct, fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct, fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct, fireworks::accounts/fireworks/models/mixtral-8x22b-instruct, fireworks::accounts/fireworks/models/mixtral-8x7b-instruct, fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct, fireworks::accounts/fireworks/models/qwen2p5-72b-instruct, hyperbolic::meta-llama/Llama-3.2-90B-Vision, hyperbolic::meta-llama/Meta-Llama-3.1-405B-FP8\n",
      "\n",
      "Text (w/ suffix): deepseek-beta::deepseek-coder, openai::gpt-3.5-turbo-instruct\n"
     ]
    }
   ],
   "source": [
    "# show models for each mode, with form `{provider}:{id}`\n",
    "if demo:\n",
    "\tprint('Chat: ' + ', '.join(chat_models))\n",
    "\tprint('\\nText: ' + ', '.join(text_models))\n",
    "\tprint('\\nText (w/ suffix): ' + ', '.join(fim_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c60a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscramble the word: s s r r k a y p c e\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct:free>\n",
      "The unscrambled word is: SPARKY\n",
      "\n",
      "<openai::gpt-4o>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct:free>\n",
      "The unscrambled word is: sparkly\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct>\n",
      "The unscrambled word is: CPRKASYR\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5>\n",
      "The unscrambled word is **CARPESSY**.\n",
      "\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5-8b>\n",
      "The unscrambled word is **PARKSYRCPE**.\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct:free>\n",
      "Unscrambled, the word is: skyscraper\n",
      "\n",
      "<openrouter::mistralai/mistral-nemo>\n",
      "The unscrambled word is \"skiracy\".\n",
      "\n",
      "<groq::llama3-groq-8b-8192-tool-use-preview>\n",
      "The unscrambled word is: scepter.\n",
      "\n",
      "<groq::llama3-groq-70b-8192-tool-use-preview>\n",
      "The unscrambled version of the letters \"s s r r k a y p c e\" is \"squirrel cage\".\n",
      "\n",
      "<openai::gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<groq::llama-3.1-8b-instant>\n",
      "The unscrambled word is: crashoppersy or a much more likely solution:  cherry cake spots or another option is sky racer pups although, \"SPRACYRK SO ASP KCYKREHRSER ASP SKY E ACPR\" another option;  one possible solution:  scyther or\n",
      "\n",
      "<openai::gpt-4o-mini>\n",
      "The unscrambled word is \"sparkery.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct>\n",
      "After unscrambling the letters, I think the unscrambled word is: \"crackers payday\" -> no, that's not a valid idiom... Wait, I have it: \"cracks payday\" -> no... Ah, got it: \"yellow peaches\" -> no... \"pay cheeks\n",
      "\n",
      "<groq::llama-3.1-70b-versatile>\n",
      "The unscrambled word is \"cryptography\" and then \"scrapy\" but an alternative correct unscrambled word from those letters is  \"cryptography\" changed slightly to  'cryptography' is removed to be replaced with - \"crazy spark\" & 'sparkcry\" & ' sparck\n",
      "\n",
      "<openrouter::mistralai/mistral-large>\n",
      "The unscrambled word is \"scribed.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-8b-instruct>\n",
      "The unscrambled word is: scrutiny pacey or scapery but most likely: specacy or key scarp or key scrappy but likely the best answer is: scrappy keys\n",
      "\n",
      "<openrouter::google/gemini-pro-1.5>\n",
      "skyscraper\n",
      "\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct>\n",
      "The unscrambled word is: ScrapYards and send back with : paceysackers\n",
      "\n",
      "<openrouter::openai/gpt-4o-mini>\n",
      "The unscrambled word is \"sparkly\".\n",
      "\n",
      "<openrouter::mistralai/codestral-mamba>\n",
      "The scrambled word \"s s r r k a y p c e\" unscrambles to \"computer\". The word is a common piece of technology used for various devices and tasks on the internet.\n",
      "\n",
      "<openrouter::openai/gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"skycrapers.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-90b-vision-instruct>\n",
      "The unscrambled word is: spacecraft keyboard sparking... no... \"spaceskyrack\" ... no... \n",
      "\n",
      " Wait! I have it!\n",
      "\n",
      "The unscrambled word is: spaceship cry... no... SPACE SKY RCARK... no... Spaces yark... \n",
      "\n",
      " Ah!\n",
      "\n",
      "The unscrambled word is\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct>\n",
      "After rearranging the letters, I think the unscrambled word is:\n",
      "\n",
      "\"scrappyorkace\"... no, wait, that doesn't look right.\n",
      "\n",
      "\"Clubscrapsy\"... nope, not that either.\n",
      "\n",
      ".. Ah ha!\n",
      "\n",
      "The unscrambled word is: ARyrscPsyak\n",
      "\n",
      "<openrouter::openai/gpt-4o>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::anthropic/claude-3.5-sonnet>\n",
      "The unscrambled word is:\n",
      "\n",
      "RASPBERRY\n",
      "\n",
      "This word is formed by rearranging all the letters in \"s s r r k a y p c e\".\n",
      "\n",
      "<openrouter::qwen/qwen-2.5-72b-instruct>\n",
      "The unscrambled word is \"characterization.\" However, it seems there might be a mistake in the scrambled word you provided, as \"ssrrkaypec\" does not form a valid English word. Could you please double-check the letters and try again? If the word is meant to be shorter or different\n",
      "\n",
      "<openrouter::nousresearch/hermes-3-llama-3.1-405b>\n",
      "The unscrambled word is:\n",
      "\n",
      "sprayers\n",
      "\n",
      "A sprayer is a device used to apply liquid substances such as pesticides, fertilizers, or cleaning solutions in the form of a fine mist or spray.\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      "The unscrambled word from the letters \"s s r r k a y p c e\" is **\"scrappy\"**.\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-70b-instruct>\n",
      "After some effort, I think I've unscrambled the word:\n",
      "\n",
      "\"s s r r k a y p c e\"\n",
      "\n",
      "Unscrambled: \"crapskayers\" ... wait, no...\n",
      "\n",
      "Unscrambled: \"askyserver\" ... nope...\n",
      "\n",
      "Unscrambled: \"serverasks\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct>\n",
      "The unscrambled word is: kayak spray or ' spray kayak sharper kayak' is made also however ' kayak spray sharper askers kayak spray' can be sharper ask sprayed maybe sharper kayak spray ask Type depending   kayak .myapplication kayak spray' \n",
      "One other possible Anagram Spray Kayaker\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-405B-Instruct>\n",
      "The unscrambled word is: Sparkcress and scrappy; however, the closest possible match is ' Sparkcress and scrappy ' isn't common, so potentially  - ' Sparcress Crayss and scrappy ' - not correct - still after trying several more attempts. By hence trying every\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a query\n",
    "if demo:\n",
    "\timport random\n",
    "\tsecret_word = 'skyscraper'\n",
    "\tscrambled = random.sample(secret_word, len(secret_word))\n",
    "\tquery = f\"Unscramble the word: {' '.join(scrambled)}\"\n",
    "\tprint(query)\n",
    "\tresults = chat_parade(query, model=chat_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b223fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct>\n",
      " make you a proposition. You can become a servant in one of my temples, serving the great goddess, Miriam. In return, I will grant you a place of safety, food, and shelter, as you grow closer to the goddess.\"\n",
      "\n",
      "Ara turned the offer over in her mind, weighing the pros and cons\n",
      "\n",
      "<openai::davinci-002>\n",
      " fix the finances, the military, the medical system. Then I've done it all. Now people are going to force another empire just to say 'hey, I'm hungry, where's my food?\" It's really an ungrateful world we live in. Anyway, it's the 50th anniversary of landing\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct>\n",
      " make sure that all people have equal rights, no matter what their religion or background.\n",
      "No one will be above the law, and everyone will have to follow the same rules and regulations.\n",
      "Free speech and assembly will be cherished and protected, and no one will be punished for expressing their opinions.\n",
      "Guns will be strictly regulated\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x7b-instruct>\n",
      " make all decisions for the people of this land. There will be no arguments, no dissent, and no second-guessing our leadership. There will be one leader, and one voice.\n",
      "\n",
      "With the death of our beloved king, it falls to me, his most loyal retainer, to take up\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x22b-instruct>\n",
      " create new laws that all dogs must be fed fresh vegetables and meat. Dogs will no longer eat their own kind or cat. Cat will no longer be eaten by dogs, only by tiger and lion. I promise that, as god-emperor, I will work to secure a peaceful existence between dogs and cats\n",
      "\n",
      "<openai::babbage-002>\n",
      " make a decree: Shall the men and women, the sons and daughters of the gods, earth, water and fire, the husbands and wives, the land, the men, the women and children, and everything besides, the stones and grass and colors, be trampled underfoot, only because politicians and bureaucrats and mineral\n",
      "\n",
      "<openai::gpt-3.5-turbo-instruct>\n",
      " inspire my administration to pursue the following four broad goals”:\n",
      "\n",
      "The rest of the above is at least kind of broad. I certainly don’t know exactly what I will do as god-emperor, but I did identify “four broad goals.”\n",
      "\n",
      "1. Ensure the prosperity of all citizens: As god-emperor, I\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct>\n",
      " make sure that you will be able to freely worship your god of choice.  The importance of religious freedom should be a cornerstone of any society.  I, however, will require a quick, quiet, prayer to me every morning and evening.  It's only fair, after all, that the people recognize my greatness\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct>\n",
      " conquer this thread.\n",
      "You shall not pass.\n",
      "But before you do, have a nice cup of tea (which is actually a normal amount, there being no need to specify that it's nice).\n",
      "*Slips sword on self in accident*\n",
      " Ah the classic mistake.\n",
      "Whoops did I just slip the god-emperor's\n",
      "\n",
      "<fireworks::accounts/fireworks/models/qwen2p5-72b-instruct>\n",
      " guide my empire to prosperity, enlighten the masses, and establish a society of compassion and wisdom. I shall do this not through force or fear, but through education, opportunity, and inspiration. To ensure the well-being of all my subjects, I will implement a series of progressive policies and social reforms. I will encourage\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " make sure that the people of the world are treated with the respect and dignity they deserve. I will ensure that everyone has access to basic necessities such as food, water, shelter, and healthcare. I will also promote education and knowledge, so that people can make informed decisions and improve their lives.\n",
      "\n",
      "As god-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a prompt\n",
    "if demo:\n",
    "\t# hyperbolic likes to do this fun little thing where its endpoints just don't work at all\n",
    "\tgood_text_models = [x for x in text_models if 'hyperbolic' not in x]\n",
    "\tprompt = 'As god-emperor, I will'\n",
    "\tresults = text_parade(prompt, model=good_text_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56cf68cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::gpt-3.5-turbo-instruct>\n",
      " as crows, do not\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " as the common loon, are known for their haunting calls and striking appearance. They are also known for their ability to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show suffix (aka FIM, or Fill-In-Middle) completion models\n",
    "\n",
    "# [Because][ token][izers][ generally][ break][ down][ sentences][ like][ this], it's best to leave *no* spaces after a prompt, and *one* space before a suffix\n",
    "if demo:\n",
    "\tprompt = 'birds, such'\n",
    "\tsuffix = ' pay taxes'\n",
    "\n",
    "\tresults = text_parade(prompt=prompt, model=fim_models, max_tokens=64, stream=False, suffix=suffix)\n",
    "\n",
    "# note: for more complex FIM tasks, a higher max_tokens count may be necessary to allow the model to find a coherent bridge between prompt and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "990fa59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4o -> openai::gpt-4o\n",
      "4o-mini -> openai::gpt-4o-mini\n",
      "claude -> openrouter::anthropic/claude-3.5-sonnet\n",
      "claude-3.5 -> openrouter::anthropic/claude-3.5-sonnet\n",
      "\n",
      "No unique chat model found for \"cla\". (Possible: openrouter::mistralai/codestral-mamba, openrouter::anthropic/claude-3.5-sonnet, openrouter::nousresearch/hermes-3-llama-3.1-405b)\n",
      "Hello! How can I assist you today? Feel free to ask any questions or let me know if you need help with anything.\n",
      "Hello! How can I assist you today? Feel free to ask any questions or let me know if you need help with anything.\n"
     ]
    }
   ],
   "source": [
    "# models have fuzzy matching, so you don't need to enter the whole name\n",
    "# just a character subsequence such that the model is minimal among all models of its mode containing that subsequence\n",
    "# e.g. \"4o-mini\" resolves to \"openai:gpt-4o-mini\", while \"4o\" resolves to \"openai:gpt-4o\"\n",
    "if demo:\n",
    "\tfor subseq in ['4o', '4o-mini', 'claude', 'claude-3.5']:\n",
    "\t\tr = resolve(subseq)\n",
    "\t\tprint(f\"{subseq} -> {', '.join('::'.join(x) for x in r)}\")\n",
    "\tprint()\n",
    "\n",
    "\t# an exception will be thrown if the resolution is ambiguous, as with 'cla'\n",
    "\ttry:\n",
    "\t\tprint(chat('hi!', model='cla'))  # raises exception\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\tprint(chat('hi!', model='claude-3.5-sonnet', temperature=0.0))  # works\n",
    "\n",
    "\t# of course, you can always just use the full name to avoid ambiguity\n",
    "\tprint(chat('hi!', model='openrouter::anthropic/claude-3.5-sonnet', temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c0f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object completion.<locals>.gen at 0x11b2d6ac0>\n",
      "\n",
      "\n",
      "\n",
      "The most widely accepted theory about the formation of the Moon is known as the Giant Impact Hypothesis. According to this theory, the Moon formed about 4.5 billion years ago after a Mars-sized body, often referred to as Theia, collided with the early Earth. The impact was so immense that it ejected a significant amount of debris into Earth's orbit. This debris eventually coalesced under the force of gravity to form the Moon.\n",
      "\n",
      "This theory is supported by several lines of evidence, including the composition of Moon rocks, which are similar to Earth's mantle, and computer simulations that demonstrate how such a collision could have produced a Moon with its current characteristics. Other theories, such as fission (the Moon split off from the Earth), capture (the Moon was a separate body that was captured by Earth's gravity), and co-formation (the Earth and Moon formed together as a double system), do not explain the available evidence as well as the Giant Impact Hypothesis."
     ]
    }
   ],
   "source": [
    "# note: if you set stream=True, completion functions will return a generator instead of an actual stream\n",
    "\n",
    "if demo:\n",
    "\tquestion = 'where did the moon come from?'\n",
    "\tprint(chat(question, model='gpt-4o-mini', stream=True))  # prints a generator object\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# to actually stream the output, you'll need to iterate over the generator:\n",
    "# ```\n",
    "# \tfor chunk in chat('hi!', model='gpt-4o-mini', stream=True):\n",
    "# \t\tprint(chunk, end='')\n",
    "# ```\n",
    "# use print_stream instead of print and this will be done for you\n",
    "\n",
    "print_stream(chat(question, model='gpt-4o-mini', stream=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8f4d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<assistant> Knock knock!\n",
      "<user> Who's there?\n",
      "<assistant> Lettuce.\n",
      "<user> Lettuce who?\n",
      "\n",
      "==========\n",
      "{\"message_history\": [{\"role\": \"assistant\", \"content\": \"Knock knock!\"}, {\"role\": \"user\", \"content\": \"Who's there?\"}, {\"content\": \"Lettuce.\", \"refusal\": null, \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}, {\"role\": \"user\", \"content\": \"Lettuce who?\"}], \"apiParams\": {\"model\": \"gpt-4o-mini\", \"suffix\": null, \"max_tokens\": null, \"stream\": null, \"n\": null, \"logprobs\": null, \"top_logprobs\": null, \"logit_bias\": null, \"temperature\": null, \"presence_penalty\": null, \"frequency_penalty\": null, \"repetition_penalty\": null, \"top_p\": null, \"min_p\": null, \"top_k\": null, \"top_a\": null, \"tools\": null, \"tool_choice\": null, \"parallel_tool_calls\": null, \"grammar\": null, \"json_schema\": null, \"response_format\": null, \"seed\": null}, \"localParams\": {\"mode\": \"chat\", \"return_raw\": true, \"pretty_tool_calls\": false, \"provider\": null, \"force_model\": null, \"force_provider\": null, \"effect\": null, \"callback\": null, \"print_output\": true, \"yield_output\": null, \"return_output\": null, \"debug\": null, \"return_object\": null}, \"stateParams\": {\"echo\": true}}\n",
      "==========\n",
      "\n",
      "<assistant> Lettuce in, it’s freezing out here!\n",
      "\n",
      "{'role': 'assistant', 'content': 'Knock knock!'}\n",
      "{'content': 'Lettuce in, it’s freezing out here!', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}\n",
      "[{'role': 'assistant', 'content': 'Knock knock!'}, {'role': 'user', 'content': \"Who's there?\"}, {'content': 'Lettuce.', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}, {'role': 'user', 'content': 'Lettuce who?'}, {'content': 'Lettuce in, it’s freezing out here!', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}]\n"
     ]
    }
   ],
   "source": [
    "# the StatefulChat class keeps track of the conversation state, and allows for saving and loading (as a string, with save_string(), load_string() or to a file, with save(path), load(path))\n",
    "# chain the methods StatefulChat.system, StatefulChat.assistant, StatefulChat.user to add messages to the conversation\n",
    "if demo:\n",
    "\tS = StatefulChat(model='gpt-4o-mini', echo=True, print_output=True)\n",
    "\tS.assistant(\"Knock knock!\").user(\"Who's there?\")\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tS.next()\n",
    "\n",
    "\tS.user(re.sub(r'[!\\.]?$', ' who?', S.last.content, 1))  # \"Xyz.\" -> \"Xyz who?\"\n",
    "\n",
    "\tdata = S.save_string()\n",
    "\tprint('\\n' + '=' * 10 + '\\n' + str(data) + '\\n' + '=' * 10 + '\\n')\n",
    "\n",
    "\tT = StatefulChat()\n",
    "\tT.load_string(data)\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tT.next()\n",
    "\n",
    "\tprint()\n",
    "\t# some convenient properties:\n",
    "\tprint(T.first)\n",
    "\tprint(T.last)\n",
    "\tprint(T.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51fd32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling run_python with arguments {'code': 'import sympy\\n\\n# Function to calculate the sum of the first n prime numbers\\ndef sum_of_primes(n):\\n    return sum(sympy.prime(i) for i in range(1, n + 1))\\n\\n# Calculate the sum of the first 2024 prime numbers\\nsum_of_primes(2024)'}\n",
      "The sum of the first 2,024 prime numbers is 16,694,571.\n",
      "Calling yield_control with arguments {'message': 'If you have any other questions or need further assistance, feel free to ask!'}\n"
     ]
    }
   ],
   "source": [
    "# StatefulChat can also be used to give an LLM access to an autonomous tool mode\n",
    "# - several tools are included; wrap them into a 'Toolbox' object to provide them to the LLM\n",
    "# - tool_calls.py contains some examples of custom tools\n",
    "# the @gatekeep decorator can be used to have a second LLM block unsatisfactory queries\n",
    "# - e.g. run_python is gatekept, and will automatically detect and refuse to run dangerous or bugged code\n",
    "# sometimes the bot will escape symbols (like \"\\\\n\" instead of \"\\n\"), which triggers an error (usually about an invalid line continuation);\n",
    "# - this is rare enough that you can generally just rerun the cell\n",
    "\n",
    "if demo:\n",
    "\ttool_prompt = \"You are an AI agent capable of using a variety of tools for looking up information and executing scripts. You are currently using these tools in autonomous mode, where you can perform self-directed, in-depth research and analysis, as well as deploy metacognitive tools (e.g. ooda_planner, meditate) to effectively clarify, plan, and ideate. Autonomous mode will go on indefinitely until you use the `yield_control` tool in order to return input access back to the user.\"\n",
    "\n",
    "\ttoolbox = Toolbox([\n",
    "\t\trun_python,  # runs python code and returns the output\n",
    "\t\tget_contents,  # fetches the plaintext contents of a URL\n",
    "\t\texa_search,  # intelligent search for semantically relevant links\n",
    "\t\tgoogle_search,  # ordinary google search\n",
    "\t\tmeditate,  # a 'metacognitive' tool that allows the LLM to deepen its own thought process\n",
    "\t\tooda_planner,  # another such tool that allows the LLM to orient itself to a situation and plan its next move\n",
    "\t\task_human,  # allows the LLM to ask the user a question and wait for a response\n",
    "\t\tyield_control  # allows the LLM to yield control to the user, so as to end its loop when its task is complete\n",
    "\t])\n",
    "\n",
    "\tmodel = 'gpt-4o'  # many bots can use the tool format provided by Toolbox\n",
    "\n",
    "\tquestion = \"What is the sum of the first 2,024 prime numbers?\"  # answer: 16,694,571 (https://www.wolframalpha.com/input?i=sum+of+first+2024+prime+numbers)\n",
    "\n",
    "\tAgent = StatefulChat(model=model, tools=toolbox, print_output=True)\n",
    "\tAgent.system(tool_prompt)\n",
    "\tAgent.user(question)\n",
    "\tAgent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27f8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Notes on the Verses by Shen-hsiu and Hui-neng_\n",
      "According to the _Platform Sutra:_\n",
      "Hung-jen, the Fifth Patriarch, the Enlightened Master\n",
      "Shen-hsiu, the Learned Senior Monk, experienced in gradual meditation\n",
      "Hui-neng, the illiterate woodcutter from the barbarian south, suddenly enlightened\n",
      "\n",
      "Causal graph - Wikipedia :  Modern developments have extended graphical models to non-parametric analysis, and thus achieved a generality and flexibility that has transformed causal analysis in computer science, epidemiology,   and social science.\n",
      "Causal map - Wikipedia :  As tools to form and represent a consensus of expert views on “what causes what” in a subject area\n",
      "Causal model - Wikipedia :  In metaphysics, a causal model (or structural causal model) is a conceptual model that describes the causal mechanisms of a system.\n",
      "t-distributed stochastic neighbor embedding - Wikipedia :  t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research,   natural language processing, music analysis,   cancer research,   bioinformatics,   geological domain interpretation,         and biomedical signal processing.\n",
      "Exploratory causal analysis - Wikipedia :  Causal analysis is the field of experimental design and statistical analysis pertaining to establishing cause and effect.\n",
      "Causality (book) - Wikipedia :  It is considered to have been instrumental in laying the foundations of the modern debate on causal inference in several fields including statistics, computer science and epidemiology.\n",
      "Path analysis (statistics) - Wikipedia :  This includes models equivalent to any form of multiple regression analysis, factor analysis, canonical correlation analysis, discriminant analysis, as well as more general families of models in the multivariate analysis of variance and covariance analyses (MANOVA, ANOVA, ANCOVA).\n",
      "Heat map - Wikipedia :  Heat maps are a way to analyze a company's existing data and update it to reflect growth and other specific efforts.\n",
      "Graphical model - Wikipedia :  Generally, probabilistic graphical models use a graph-based representation as the foundation for encoding a distribution over a multi-dimensional space and a graph that is a compact or factorized representation of a set of independences that hold in the specific distribution.\n",
      "Structural equation modeling - Wikipedia :  Structural equation modeling (SEM) is a label for a diverse set of methods used by scientists in both experimental and observational research across the sciences,   business,   and other fields.\n"
     ]
    }
   ],
   "source": [
    "# you can also use these tools yourself\n",
    "if demo:\n",
    "\t# get_contents parses the contents of a given URL and returns just the plaintext content\n",
    "\t# - <div class=\"note\"><div class=\"parenthetical\"><p>(without all the <a href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a> bloat)</p></div></div>\n",
    "\turl_1 = \"https://pages.uoregon.edu/munno/OregonCourses/REL444S05/HuinengVerse.htm\"\n",
    "\tformatter = lambda results: '\\n'.join(results[\"data\"][\"content\"].split('\\n\\n')[:5])  # just print the first five lines, since it's a long website\n",
    "\tresults = get_contents(url_1)\n",
    "\t# output schema: { 'code': int, 'status': int, 'data': { 'title': str, 'description': str, 'url': str, 'content': str, 'usage': { 'tokens': int } } }\n",
    "\tprint(formatter(results))\n",
    "\tprint()\n",
    "\n",
    "\t# exa_search finds websites related to a given URL\n",
    "\turl_2 = \"https://en.wikipedia.org/wiki/Causal_map\"\n",
    "\tresults = exa_search(url_2)\n",
    "\t# output schema: [ { 'url': str, 'title': str, 'date': str, 'snippet': list[str] } ]\n",
    "\tfor result in results:\n",
    "\t\tprint(result['title'], ': ', result['snippet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21154f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling get_weather with arguments {'city': 'Portland'}\n",
      "Today is not a good day for hiking in Portland. The current weather includes a hailstorm, with a temperature of 21°F and wind speeds at 35 mph. These conditions are not safe for outdoor activities like hiking. It's best to wait for better weather.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also define your own tools by directly writing them as Python functions\n",
    "# so long as they're type-hinted and have a docstring of the appropriate format, the @Tool decorator will automatically wrap them into a format suitable for use with an LLM\n",
    "if demo:\n",
    "\t# example tool: get the weather for a given city\n",
    "\t# not hooked up to a weather API, though, so it just gives a hardcoded answer\n",
    "\t@Tool\n",
    "\tdef get_weather(city: str) -> Dict[str, Union[float, str]]:\n",
    "\t\t\"\"\"\n",
    "\t\tGet the current weather in a given city, in °F and mph.\n",
    "\t\t:param city: The name of the city.\n",
    "\t\t:returns: A dictionary containing the current temperature (°F), weather condition, and wind speed (mph) in the given city.\n",
    "\t\t\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'temperature': 21.0,\n",
    "\t\t\t'condition': 'hailstorm',\n",
    "\t\t\t'wind_speed': 35.0\n",
    "\t\t}\n",
    "\n",
    "\tquery_params = Dict({\"activity\": \"go hiking\", \"city\": \"Portland\"})\n",
    "\tweather_query = f\"Is today a good day to {query_params.activity} in {query_params.city}?\"\n",
    "\tweather_tools = Toolbox([get_weather])\n",
    "\tweather_model = \"gpt-4o\"\n",
    "\n",
    "\tWeatherAdvisor = StatefulChat(model=weather_model, tools=weather_tools, print_output=True)\n",
    "\tWeatherAdvisor.user(weather_query)\n",
    "\tWeatherAdvisor.next()\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64354bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoughts: None\n",
      "\n",
      "explanation: The Diels-Alder reaction is a chemical reaction that is important in the field of organic chemistry because it allows scientists to build complex carbon-based structures efficiently. Although it might seem intricate at first glance, it is a fundamental tool that enables the construction of important chemical compounds, such as natural products or pharmaceuticals. This reaction forms a six-membered ring by combining two separate molecules. \n",
      "\n",
      "answer: The Diels-Alder reaction is a chemical reaction between a conjugated diene and a dienophile, resulting in the formation of a six-membered ring.\n",
      "\n",
      "example: Imagine you have two Lego bricks: a long piece (representing the diene) and a shorter piece with a bump (representing the dienophile). The Diels-Alder reaction would be like snapping them together to create a new, more complex structure (the six-membered ring).\n",
      "\n",
      "notes: - The \"diene\" is a molecule with two alternating double bonds.\n",
      "- The \"dienophile\" is typically a molecule with a double or triple bond that is ready to react with the diene.\n",
      "- The reaction is known for being highly stereospecific, meaning the spatial arrangement of atoms in the products is largely predictable.\n",
      "- It is a type of cycloaddition reaction, specifically known as a [4+2] cycloaddition.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structured Outputs (GPT-4o, 4o-mini)\n",
    "# ensures that the completion is always a valid JSON object with a schema that you define\n",
    "# Enable by passing a dict \"response_format\" = {\"type\": \"json_schema\", \"json_schema\": (your schema)}. In this schema,\n",
    "# - additionalProperties must be false, strict must be true, and required must contain all properties\n",
    "# - the types string, number, integer, boolean, object, array, null, and enum are supported\n",
    "# - union types are supported, and are represented as arrays of types; in particular, you can make a param 'optional' by making its type [\"string\", \"null\"]\n",
    "# - references are supported, and can point to either subschemas ({\"$ref\": \"#/$defs/property\"} or recurse ({\"$ref\": \"#\"})\n",
    "\n",
    "# Example: Schema for answering a question\n",
    "if demo:\n",
    "    import json\n",
    "    with open('data/structured_outputs/answer_question.json') as file:\n",
    "        answer_schema = json.load(file)\n",
    "    # the json object will be returned as a string (serialized), so json.loads should be used to turn it into a schema-compatible dictionary\n",
    "    # streaming is incompatible with the Structured Outputs feature, so it might take a bit for the finished answer to appear\n",
    "    answer_creator = lambda q: json.loads(chat_completion(q, {\"model\": \"gpt-4o\", \"response_format\": {\"type\": \"json_schema\", \"json_schema\": answer_schema}}))\n",
    "    my_question = \"What is a Diels-Alder reaction? Please explain like I only know the basics of organic chemistry.\"\n",
    "\n",
    "    for k, v in answer_creator(my_question).items():\n",
    "        print(f\"{k}: {v}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
