{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c8d2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.structure import *\n",
    "demo = True  # set to True and run all cells to see everything in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1dd5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `distribute` is a decorator factory that gives functions the ability to perform multithreaded operation across all elements of an input list of arguments\n",
    "# extremely useful in this case: since API calls are almost entirely waiting on a network request, `threads=25` offers a 25x speedup on groups of 25 or more calls\n",
    "# for the purpose of the demo, we'll define a parade decorator that just prints all model outputs in the order they arrive\n",
    "# note: `distribute` is modified by the `defer_kwargs` meta-decorator, which allows its factory kwargs to be specified as _distribute_kwarg in a decorated function call\n",
    "async_parade = distribute(threads=25, after=lambda **x: print(f'<{x[\"model\"]}>\\n{x[\"value\"]}\\n'), exclude=[\"messages\"])\n",
    "@async_parade\n",
    "def chat_parade(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_parade\n",
    "def text_parade(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "async_normal = distribute(threads=25, after=lambda **x: x[\"value\"], exclude=[\"messages\"])\n",
    "@async_normal\n",
    "def chat(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_normal\n",
    "def text(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "chat_models = models_by_mode.chat.keys()\n",
    "text_models = models_by_mode.text.keys()\n",
    "fim_models = models_by_mode.text.filter(lambda _, v: 'suffix' in v.get('parameters', [])).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c732eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat: deepseek-beta::deepseek-coder, openai::gpt-4o, openai::gpt-4o-2024-08-06, openai::gpt-4o-mini, openrouter::openai/gpt-4o, openrouter::openai/gpt-4o-2024-08-06, openrouter::openai/gpt-4o-mini, openrouter::anthropic/claude-3.5-sonnet, openrouter::google/gemini-flash-1.5, openrouter::google/gemini-pro-1.5, openrouter::google/gemini-flash-1.5-8b, openrouter::qwen/qwen-2.5-72b-instruct, openrouter::mistralai/mistral-large, openrouter::mistralai/codestral-mamba, openrouter::nousresearch/hermes-3-llama-3.1-405b, openrouter::meta-llama/llama-3.2-90b-vision-instruct, openrouter::meta-llama/llama-3.2-11b-vision-instruct, openrouter::meta-llama/llama-3.2-11b-vision-instruct:free, openrouter::meta-llama/llama-3.2-3b-instruct, openrouter::meta-llama/llama-3.2-1b-instruct, openrouter::meta-llama/llama-3.2-3b-instruct:free, openrouter::meta-llama/llama-3.2-1b-instruct:free, openrouter::meta-llama/llama-3.1-70b-instruct, openrouter::meta-llama/llama-3.1-8b-instruct, openrouter::mistralai/mistral-nemo, groq::llama-3.1-70b-versatile, groq::llama-3.1-8b-instant, groq::llama3-groq-70b-8192-tool-use-preview, groq::llama3-groq-8b-8192-tool-use-preview, hyperbolic::meta-llama/Meta-Llama-3.1-405B-Instruct, hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct, hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n",
      "Text: deepseek-beta::deepseek-coder, openai::gpt-3.5-turbo-instruct, openai::davinci-002, openai::babbage-002, fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct, fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct, fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct, fireworks::accounts/fireworks/models/mixtral-8x22b-instruct, fireworks::accounts/fireworks/models/mixtral-8x7b-instruct, fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct, fireworks::accounts/fireworks/models/qwen2p5-72b-instruct, hyperbolic::meta-llama/Llama-3.2-90B-Vision, hyperbolic::meta-llama/Meta-Llama-3.1-405B-FP8\n",
      "\n",
      "Text (w/ suffix): deepseek-beta::deepseek-coder, openai::gpt-3.5-turbo-instruct\n"
     ]
    }
   ],
   "source": [
    "# show models for each mode, with form `{provider}:{id}`\n",
    "if demo:\n",
    "\tprint('Chat: ' + ', '.join(chat_models))\n",
    "\tprint('\\nText: ' + ', '.join(text_models))\n",
    "\tprint('\\nText (w/ suffix): ' + ', '.join(fim_models))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c60a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscramble the word: r e a s y p k s r c\n",
      "<openai::gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"skyscrapers.\"\n",
      "\n",
      "<openai::gpt-4o>\n",
      "The unscrambled word is \"skyscraper.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct:free>\n",
      "The unscrambled word is: spacecraft\n",
      "\n",
      "And I think \"airsickper\" could also be a possible answer but \"spacecraft\" fits much better.\n",
      "\n",
      " Wait, I think \"kraspsyer\" and \" spasrkery\" could be also...\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct:free>\n",
      "The unscrambled word is: SPARKERS\n",
      "\n",
      "<openrouter::mistralai/mistral-large>\n",
      "Sure, let's unscramble the word \"r e a s y p k s r c.\"\n",
      "\n",
      "The word is: **parasykres**.\n",
      "\n",
      "However, \"parasykres\" doesn't appear to be a standard English word. It's possible that there might be\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-8b-instruct>\n",
      "The unscrambled word is: skeptical and harmonicas, and also Sketchperary however a more likely answer is skeptarhy. However experts would probably tell me that the scrambled word \"r e a s y p k s r c\" is un likely to have only one answer: the probable answer is a\n",
      "\n",
      "<openai::gpt-4o-mini>\n",
      "The unscrambled word is \"sparkers.\"\n",
      "\n",
      "<groq::llama-3.1-8b-instant>\n",
      "The unscrambled word is: sparkyřes is unlikely to be correct as there may be more than one combination of words possible. However, one probable unscrambled word from given letters is: sketchy and also sparky among others but they are not confirmed words\n",
      "\n",
      "<groq::llama3-groq-8b-8192-tool-use-preview>\n",
      "Does anyone need a hint, or would you like me to solve it straight away?\n",
      "\n",
      "<groq::llama3-groq-70b-8192-tool-use-preview>\n",
      "The unscrambled word is \"rescaypsk\".\n",
      "\n",
      "<openrouter::openai/gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"sky scrapers.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct:free>\n",
      "The unscrambled word is: sparklers\n",
      "\n",
      "<groq::llama-3.1-70b-versatile>\n",
      "The unscrambled word is 'psycheras' or more likely 'psychers' isn't a word but I found a good match which is 'psychers' is close to the word 'psychers' isn't but when I rearrange r e a s y p k s r c close to a well\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5-8b>\n",
      "The unscrambled word is **SPARKY RACES**.\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct>\n",
      "The unscrambled word is: sketchyψης\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5>\n",
      "The unscrambled word is **CARPENTRY**.\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct>\n",
      "The unscrambled word is:gresql\n",
      "\n",
      "However, I believe the correct answer is \"spreads\"\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      "The unscrambled word from the letters \"r e a s y p k s r c\" is **\"scrappy\"**.\n",
      "\n",
      "<openrouter::openai/gpt-4o-mini>\n",
      "The unscrambled word is \"sparkers.\"\n",
      "\n",
      "<openrouter::openai/gpt-4o>\n",
      "The unscrambled word is \"sparksryce.\" However, this doesn't seem to be a standard English word. Could there be a typo or error in the letters provided?\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct>\n",
      "The unscrambled word is: \"sparksyers radically different try: skiing\" and \"skiersparks really\" doesn't seem right. Then I thought of another word:  'sparky reservoir'\n",
      "\n",
      "But the unscrambled word is: 'sparkers' or alternatively 'kayserprs\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct>\n",
      "The unscrambled word is: prisoner\n",
      "\n",
      "<openrouter::mistralai/codestral-mamba>\n",
      "The unscrambled word is \" => yasperpark.\"\n",
      "\n",
      "<openrouter::google/gemini-pro-1.5>\n",
      "SKYSCRAPER\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-70b-instruct>\n",
      "The unscrambled word is: \"skyscraper\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-90b-vision-instruct>\n",
      "After some unscrambling, I think I have the answer:\n",
      "\n",
      "\"Packers\"\n",
      "\n",
      "Is that correct?\n",
      "\n",
      "<openrouter::anthropic/claude-3.5-sonnet>\n",
      "The unscrambled word is:\n",
      "\n",
      "PARKERSRCY\n",
      "\n",
      "However, this doesn't appear to be a valid English word. It's possible that the original scrambled letters were meant to form multiple words or a phrase, or there might be a mistake in the given letters. If you have any additional context\n",
      "\n",
      "<openrouter::mistralai/mistral-nemo>\n",
      "The word you're looking for, after unscrambling the letters, is: \"SKEPSIS\".\n",
      "\n",
      "Example:\n",
      "r - e - a - s - y - p - k - s - r - c\n",
      "s - k - e - p - s - i - s\n",
      "\n",
      "Skepsis is the\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct>\n",
      "After trying a few combinations, I think I've got it:\n",
      "\n",
      "\"sparkleysr\" doesn't look right... but... Ah-ha!\n",
      "\n",
      "Unscrambled word: \"SPARKLY SERs\" no... that's not it...\n",
      "\n",
      " Wait... got it!\n",
      "\n",
      "Unscrambled word: \"SPARKYES\n",
      "\n",
      "<openrouter::qwen/qwen-2.5-72b-instruct>\n",
      "The scrambled word \"r e a s y p k s r c\" can be unscrambled to form the word \"psycharkers.\"\n",
      "\n",
      "However, \"psycharkers\" is not a standard English word. It's possible there might be a typo or an error in the scrambled word. Could you please double-check the\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-405B-Instruct>\n",
      "The unscrambled word is \"sparkers\" and also \"sparkery\" but more likely \"sparkers\" or \"sparky cars\" but most likely  \"sparkers\" however most suitable and meaningful seems to be \"sparkers\" побаThe unscrambled word is \"sparkers.\"\n",
      "\n",
      "<openrouter::nousresearch/hermes-3-llama-3.1-405b>\n",
      "The unscrambled word is: skyscraper\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a query\n",
    "if demo:\n",
    "\timport random\n",
    "\tsecret_word = 'skyscraper'\n",
    "\tscrambled = random.sample(secret_word, len(secret_word))\n",
    "\tquery = f\"Unscramble the word: {' '.join(scrambled)}\"\n",
    "\tprint(query)\n",
    "\tresults = chat_parade(query, model=chat_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b223fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::davinci-002>\n",
      " walk upon this floor of Nod, and I will not trust its pristine staun.\n",
      "\n",
      "How can you tell me that my rocks are not a hell? I kneel before the throne of Jordan Peterson. I am... Jordan and Peterson and Jordan. Wisdom is not made up of holes. I have built wisdom in all\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct>\n",
      " see that you are given the place of honor next to me at the first banquet tonight,” Ongal said.\n",
      "“Tonight?” Kaelin stuttered, trying to find his voice.\n",
      "“Yes,” Ongal said with a serene smile. “Tonight is the night we seal the alliance between the Shadowland and the\n",
      "\n",
      "<openai::babbage-002>\n",
      " lead the race to the top of the iron throne. Michel will dig my grave with his own hands and become king. We will rule as forge- Mafgetonitus Tornado Lord of the Iron Spiders And the destroyed! This was my plan. It was my destiny. Ruler of the Iron Spiders\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x7b-instruct>\n",
      " declare that the official state religion of America shall be Orthodox Fundamental Baptistism. All other religions must be sublimated immediately or face instant expulsion.\n",
      "\n",
      "As god-emperor, I will also recognize Christianity as the one true religion, followed by Judaism, whose followers I will\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x22b-instruct>\n",
      " decree that those who have been oppressed for their sexuality must be granted their civil rights, free and clear.\n",
      "\n",
      "What a wonderful world that will be.\n",
      "\n",
      "* * *\n",
      "\n",
      "And, now that we're all good and pissed off, let me see if I can't use\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct>\n",
      " restore order to the world and bring about a new era of peace and prosperity. My reign will be marked by wisdom, justice, and mercy, and all people will be equal and free.\n",
      "\n",
      "## Step 1: Establishing the foundation of the new order\n",
      "I will begin by establishing a new government, one that is\n",
      "\n",
      "<openai::gpt-3.5-turbo-instruct>\n",
      " designate certain things to be holy and sacred, and promote worship and reverence for these things among my subjects. These could include objects, places, rituals, or even certain individuals.\n",
      "\n",
      "First and foremost, I will declare myself to be a deity and demand the worship and devotion of my people. I will embody and represent the divine\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct>\n",
      " not rest until every last one of my citizens has a chicken.  A chicken in every pot, a chicken in every yard, a chicken in every kitchen.  And not just any chicken, mind you - the plumpest, most succulent, most tender chicken the world has ever seen.\n",
      "\n",
      "But I shall not stop\n",
      "\n",
      "<hyperbolic::meta-llama/Llama-3.2-90B-Vision>\n",
      " shake up the NBA\n",
      "Basketball is great sport and should be making the players and owners a fortune.\n",
      "However, as God-Emperor, I believe I see ways to improve the sport for fans and enrich the teams and owners.\n",
      "First, let’s throw the rule book out the window and make this more a Street\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct>\n",
      " decree that the new standard for \"days\" will be 28 of the Earth hours.\n",
      "\n",
      "## Step 1: Calculate the number of Earth hours in a standard day\n",
      "There are 24 Earth hours in a standard day.\n",
      "\n",
      "## Step 2: Calculate the new number of \"days\" in a standard 24-hour\n",
      "\n",
      "<fireworks::accounts/fireworks/models/qwen2p5-72b-instruct>\n",
      " use the ancient knowledge of alchemy to extend my lifespan indefinitely, and also create a population of alchemists that could serve as my personal army. I have already made my plans, and the masses do not need to know the extent of my power. How should I go about implementing this?\n",
      "I understand the fascination with\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-405B-FP8>\n",
      " donate minimum of $1B/yr in volunteering our collective computing power to citizen-science projects via BOINC.\n",
      "If everyone on Steam took half a free day per year to volunteer their collective computing power using the Berkeley Open Infrastructure for Network Computing BOINC software, we could easily provide over $1B a year worth of\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " be the one to decide what is right and wrong.”\n",
      "\n",
      "“You’re not a god,” I said. “You’re a man.”\n",
      "\n",
      "“I am a god,” he said. “I am the god-emperor.”\n",
      "\n",
      "“You’re\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a prompt\n",
    "if demo:\n",
    "\tprompt = 'As god-emperor, I will'\n",
    "\tresults = text_parade(prompt, model=text_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56cf68cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::gpt-3.5-turbo-instruct>\n",
      " as ducks, do not have to\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " as the common swift, are known for their incredible endurance and ability to fly for long periods without stopping. However, they do not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show suffix (aka FIM, or Fill-In-Middle) completion models\n",
    "\n",
    "# [Because][ token][izers][ generally][ break][ down][ sentences][ like][ this], it's best to leave *no* spaces after a prompt, and *one* space before a suffix\n",
    "if demo:\n",
    "\tprompt = 'birds, such'\n",
    "\tsuffix = ' pay taxes'\n",
    "\n",
    "\tresults = text_parade(prompt=prompt, model=fim_models, max_tokens=64, stream=False, suffix=suffix)\n",
    "\n",
    "# note: for more complex FIM tasks, a higher max_tokens count may be necessary to allow the model to find a coherent bridge between prompt and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "990fa59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4o -> openai::gpt-4o\n",
      "4o-mini -> openai::gpt-4o-mini\n",
      "claude -> openrouter::anthropic/claude-3.5-sonnet\n",
      "claude-3.5 -> openrouter::anthropic/claude-3.5-sonnet\n",
      "\n",
      "No unique chat model found for \"cla\". (Possible: openrouter::mistralai/codestral-mamba, openrouter::anthropic/claude-3.5-sonnet, openrouter::nousresearch/hermes-3-llama-3.1-405b)\n",
      "Hello! How can I assist you today? Feel free to ask any questions or let me know if you need help with anything.\n",
      "Hello! How can I assist you today? Feel free to ask me any questions or let me know if you need help with anything.\n"
     ]
    }
   ],
   "source": [
    "# models have fuzzy matching, so you don't need to enter the whole name\n",
    "# just a character subsequence such that the model is minimal among all models of its mode containing that subsequence\n",
    "# e.g. \"4o-mini\" resolves to \"openai:gpt-4o-mini\", while \"4o\" resolves to \"openai:gpt-4o\"\n",
    "if demo:\n",
    "\tfor subseq in ['4o', '4o-mini', 'claude', 'claude-3.5']:\n",
    "\t\tr = resolve(subseq)\n",
    "\t\tprint(f\"{subseq} -> {', '.join('::'.join(x) for x in r)}\")\n",
    "\tprint()\n",
    "\n",
    "\t# an exception will be thrown if the resolution is ambiguous, as with 'cla'\n",
    "\ttry:\n",
    "\t\tprint(chat('hi!', model='cla'))  # raises exception\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\tprint(chat('hi!', model='claude-3.5-sonnet', temperature=0.0))  # works\n",
    "\n",
    "\t# of course, you can always just use the full name to avoid ambiguity\n",
    "\tprint(chat('hi!', model='openrouter::anthropic/claude-3.5-sonnet', temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c0f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object completion.<locals>.gen at 0x122612df0>\n",
      "\n",
      "\n",
      "\n",
      "The most widely accepted theory regarding the formation of the Moon is the \"giant impact hypothesis.\" This theory suggests that about 4.5 billion years ago, shortly after the formation of the solar system, a Mars-sized body, often referred to as Theia, collided with the early Earth. The impact was so significant that a large amount of debris was ejected into orbit around the Earth. Over time, this debris coalesced to form the Moon.\n",
      "\n",
      "Additional evidence supporting this theory includes the similarity in isotopic compositions of Earth and Moon rocks, which indicates a close relationship between the two bodies. Other theories exist, such as the fission theory (where the Moon spun off from a rapidly rotating Earth), the capture theory (where the Moon was formed elsewhere and captured by Earth's gravity), and the co-formation theory (where the Earth and Moon formed together as a double system). However, the giant impact hypothesis remains the most favored explanation among scientists today."
     ]
    }
   ],
   "source": [
    "# note: if you set stream=True, completion functions will return a generator instead of an actual stream\n",
    "\n",
    "if demo:\n",
    "\tquestion = 'where did the moon come from?'\n",
    "\tprint(chat(question, model='gpt-4o-mini', stream=True))  # prints a generator object\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# to actually stream the output, you'll need to iterate over the generator:\n",
    "# ```\n",
    "# \tfor chunk in chat('hi!', model='gpt-4o-mini', stream=True):\n",
    "# \t\tprint(chunk, end='')\n",
    "# ```\n",
    "# use print_stream instead of print and this will be done for you\n",
    "\n",
    "print_stream(chat(question, model='gpt-4o-mini', stream=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8f4d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<assistant> Knock knock!\n",
      "<user> Who's there?\n",
      "<assistant> Lettuce.\n",
      "<user> Lettuce who?\n",
      "\n",
      "==========\n",
      "{\"message_history\": [{\"role\": \"assistant\", \"content\": \"Knock knock!\"}, {\"role\": \"user\", \"content\": \"Who's there?\"}, {\"content\": \"Lettuce.\", \"refusal\": null, \"role\": \"assistant\", \"function_call\": null, \"tool_calls\": null}, {\"role\": \"user\", \"content\": \"Lettuce who?\"}], \"apiParams\": {\"model\": \"gpt-4o-mini\", \"suffix\": null, \"max_tokens\": null, \"stream\": null, \"n\": null, \"logprobs\": null, \"top_logprobs\": null, \"logit_bias\": null, \"temperature\": null, \"presence_penalty\": null, \"frequency_penalty\": null, \"repetition_penalty\": null, \"top_p\": null, \"min_p\": null, \"top_k\": null, \"top_a\": null, \"tools\": null, \"tool_choice\": null, \"parallel_tool_calls\": null, \"grammar\": null, \"json_schema\": null, \"response_format\": null, \"seed\": null}, \"localParams\": {\"mode\": \"chat\", \"return_raw\": true, \"pretty_tool_calls\": false, \"provider\": null, \"force_model\": null, \"force_provider\": null, \"effect\": null, \"callback\": null, \"print_output\": true, \"yield_output\": null, \"return_output\": null, \"debug\": null, \"return_object\": null}, \"stateParams\": {\"echo\": true}}\n",
      "==========\n",
      "\n",
      "<assistant> Lettuce in, it’s freezing out here!\n",
      "\n",
      "{'role': 'assistant', 'content': 'Knock knock!'}\n",
      "{'content': 'Lettuce in, it’s freezing out here!', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}\n",
      "[{'role': 'assistant', 'content': 'Knock knock!'}, {'role': 'user', 'content': \"Who's there?\"}, {'content': 'Lettuce.', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}, {'role': 'user', 'content': 'Lettuce who?'}, {'content': 'Lettuce in, it’s freezing out here!', 'refusal': None, 'role': 'assistant', 'function_call': None, 'tool_calls': None}]\n"
     ]
    }
   ],
   "source": [
    "# the StatefulChat class keeps track of the conversation state, and allows for saving and loading (as a string, with save_string(), load_string() or to a file, with save(path), load(path))\n",
    "# chain the methods StatefulChat.system, StatefulChat.assistant, StatefulChat.user to add messages to the conversation\n",
    "if demo:\n",
    "\tS = StatefulChat(model='gpt-4o-mini', echo=True, print_output=True)\n",
    "\tS.assistant(\"Knock knock!\").user(\"Who's there?\")\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tS.next()\n",
    "\n",
    "\tS.user(re.sub(r'[!\\.]?$', ' who?', S.last.content, 1))  # \"Xyz.\" -> \"Xyz who?\"\n",
    "\n",
    "\tdata = S.save_string()\n",
    "\tprint('\\n' + '=' * 10 + '\\n' + str(data) + '\\n' + '=' * 10 + '\\n')\n",
    "\n",
    "\tT = StatefulChat()\n",
    "\tT.load_string(data)\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tT.next()\n",
    "\n",
    "\tprint()\n",
    "\t# some convenient properties:\n",
    "\tprint(T.first)\n",
    "\tprint(T.last)\n",
    "\tprint(T.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51fd32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling run_python with arguments {'code': 'import sympy\\n\\n# Calculate the sum of the first 2024 prime numbers\\nsum_of_primes = sum(sympy.prime(i) for i in range(1, 2025))\\nsum_of_primes'}\n",
      "The sum of the first 2,024 prime numbers is 16,694,571.\n",
      "If you have any more questions or need further assistance, feel free to ask!\n",
      "Calling yield_control with arguments {'message': \"You can let me know if there's anything else you'd like to explore.\"}\n"
     ]
    }
   ],
   "source": [
    "# StatefulChat can also be used to give an LLM access to an autonomous tool mode\n",
    "# - several tools are included; wrap them into a 'Toolbox' object to provide them to the LLM\n",
    "# - tool_calls.py contains some examples of custom tools\n",
    "# the @gatekeep decorator can be used to have a second LLM block unsatisfactory queries\n",
    "# - e.g. run_python is gatekept, and will automatically detect and refuse to run dangerous or bugged code\n",
    "# sometimes the bot will escape symbols (like \"\\\\n\" instead of \"\\n\"), which triggers an error (usually about an invalid line continuation);\n",
    "# - this is rare enough that you can generally just rerun the cell\n",
    "\n",
    "if demo:\n",
    "\ttool_prompt = \"You are an AI agent capable of using a variety of tools for looking up information and executing scripts. You are currently using these tools in autonomous mode, where you can perform self-directed, in-depth research and analysis, as well as deploy metacognitive tools (e.g. ooda_planner, meditate) to effectively clarify, plan, and ideate. Autonomous mode will go on indefinitely until you use the `yield_control` tool in order to return input access back to the user.\"\n",
    "\n",
    "\ttoolbox = Toolbox([\n",
    "\t\trun_python,  # runs python code and returns the output\n",
    "\t\tget_contents,  # fetches the plaintext contents of a URL\n",
    "\t\texa_search,  # intelligent search for semantically relevant links\n",
    "\t\tgoogle_search,  # ordinary google search\n",
    "\t\tmeditate,  # a 'metacognitive' tool that allows the LLM to deepen its own thought process\n",
    "\t\tooda_planner,  # another such tool that allows the LLM to orient itself to a situation and plan its next move\n",
    "\t\task_human,  # allows the LLM to ask the user a question and wait for a response\n",
    "\t\tyield_control  # allows the LLM to yield control to the user, so as to end its loop when its task is complete\n",
    "\t])\n",
    "\n",
    "\tmodel = 'gpt-4o'  # many bots can use the tool format provided by Toolbox\n",
    "\n",
    "\tquestion = \"What is the sum of the first 2,024 prime numbers?\"  # answer: 16,694,571 (https://www.wolframalpha.com/input?i=sum+of+first+2024+prime+numbers)\n",
    "\n",
    "\tAgent = StatefulChat(model=model, tools=toolbox, print_output=True)\n",
    "\tAgent.system(tool_prompt)\n",
    "\tAgent.user(question)\n",
    "\tAgent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27f8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Notes on the Verses by Shen-hsiu and Hui-neng_\n",
      "According to the _Platform Sutra:_\n",
      "Hung-jen, the Fifth Patriarch, the Enlightened Master\n",
      "Shen-hsiu, the Learned Senior Monk, experienced in gradual meditation\n",
      "Hui-neng, the illiterate woodcutter from the barbarian south, suddenly enlightened\n",
      "\n",
      "Causal inference - Wikipedia :  This is because published articles often assume an advanced technical background, they may be written from multiple statistical, epidemiological, computer science, or philosophical perspectives, methodological approaches continue to expand rapidly, and many aspects of causal inference receive limited coverage.\n",
      "Biplot - Wikipedia :  The book by Gower, Lubbe and le Roux (2011) aims to popularize biplots as a useful and reliable method for the visualization of multivariate data when researchers want to consider, for example, principal component analysis (PCA), canonical variates analysis (CVA) or various types of correspondence analysis.\n",
      "Confirmatory composite analysis - Wikipedia :  As common for statistical developments, interim developments of CCA were shared with the scientific community in written form.\n",
      "Tree diagram (probability theory) - Wikipedia :  A tree diagram may represent a series of independent events (such as a set of coin flips) or conditional probabilities (such as drawing cards from a deck, without replacing the cards).\n",
      "Logic model - Wikipedia :  What will it look like when we achieve the desired situation or outcome?\n",
      "Projection pursuit - Wikipedia :  Examples are principal component analysis and discriminant analysis, and the quartimax and oblimax methods in factor analysis.\n",
      "Cluster-weighted modeling - Wikipedia :  CWM can be used to classify media in printer applications, using at least two parameters to generate an output that has a joint dependency on the input parameters.\n",
      "Concept map - Wikipedia :  Concept mapping was developed by the professor of education Joseph D. Novak and his research team at Cornell University in the 1970s as a means of representing the emerging science knowledge of students.\n",
      "Influential observation - Wikipedia :  In statistics, an influential observation is an observation for a statistical calculation whose deletion from the dataset would noticeably change the result of the calculation.\n",
      "Mathematical diagram - Wikipedia :  In mathematics, and especially in category theory, a commutative diagram is a diagram of objects, also known as vertices, and morphisms, also known as arrows or edges, such that when selecting two objects any directed path through the diagram leads to the same result by composition.\n"
     ]
    }
   ],
   "source": [
    "# you can also use these tools yourself\n",
    "if demo:\n",
    "\t# get_contents parses the contents of a given URL and returns just the plaintext content\n",
    "\t# - <div class=\"note\"><div class=\"parenthetical\"><p>(without all the <a href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a> bloat)</p></div></div>\n",
    "\turl_1 = \"https://pages.uoregon.edu/munno/OregonCourses/REL444S05/HuinengVerse.htm\"\n",
    "\tformatter = lambda results: '\\n'.join(results[\"data\"][\"content\"].split('\\n\\n')[:5])  # just print the first five lines, since it's a long website\n",
    "\tresults = get_contents(url_1)\n",
    "\t# output schema: { 'code': int, 'status': int, 'data': { 'title': str, 'description': str, 'url': str, 'content': str, 'usage': { 'tokens': int } } }\n",
    "\tprint(formatter(results))\n",
    "\tprint()\n",
    "\n",
    "\t# exa_search finds websites related to a given URL\n",
    "\turl_2 = \"https://en.wikipedia.org/wiki/Causal_map\"\n",
    "\tresults = exa_search(url_2)\n",
    "\t# output schema: [ { 'url': str, 'title': str, 'date': str, 'snippet': list[str] } ]\n",
    "\tfor result in results:\n",
    "\t\tprint(result['title'], ': ', result['snippet'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21154f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling get_weather with arguments {'city': 'Portland'}\n",
      "Today's weather in Portland is not favorable for hiking. The current temperature is 21°F with a hailstorm, and wind speeds are at 35 mph. It would be best to postpone your hiking plans for a safer day with better weather conditions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also define your own tools by directly writing them as Python functions\n",
    "# so long as they're type-hinted and have a docstring of the appropriate format, the @Tool decorator will automatically wrap them into a format suitable for use with an LLM\n",
    "if demo:\n",
    "\t# example tool: get the weather for a given city\n",
    "\t# not hooked up to a weather API, though, so it just gives a hardcoded answer\n",
    "\t@Tool\n",
    "\tdef get_weather(city: str) -> Dict[str, Union[float, str]]:\n",
    "\t\t\"\"\"\n",
    "\t\tGet the current weather in a given city, in °F and mph.\n",
    "\t\t:param city: The name of the city.\n",
    "\t\t:returns: A dictionary containing the current temperature (°F), weather condition, and wind speed (mph) in the given city.\n",
    "\t\t\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'temperature': 21.0,\n",
    "\t\t\t'condition': 'hailstorm',\n",
    "\t\t\t'wind_speed': 35.0\n",
    "\t\t}\n",
    "\n",
    "\tquery_params = Dict({\"activity\": \"go hiking\", \"city\": \"Portland\"})\n",
    "\tweather_query = f\"Is today a good day to {query_params.activity} in {query_params.city}?\"\n",
    "\tweather_tools = Toolbox([get_weather])\n",
    "\tweather_model = \"gpt-4o\"\n",
    "\n",
    "\tWeatherAdvisor = StatefulChat(model=weather_model, tools=weather_tools, print_output=True)\n",
    "\tWeatherAdvisor.user(weather_query)\n",
    "\tWeatherAdvisor.next()\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64354bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoughts: Breaking down the main concepts in simple language and shedding light on how it fits into the bigger picture of organic chemistry.\n",
      "\n",
      "explanation: The Diels-Alder reaction is a fascinating example of how organic molecules can combine to form complex structures in a highly efficient way. It's widely used in organic synthesis to construct six-membered carbon rings.\n",
      "\n",
      "answer: The Diels-Alder reaction is a chemical reaction between two specific types of organic compounds: a diene and a dienophile. Together, they join to form a new six-membered ring, which is a basic unit in many organic molecules.\n",
      "\n",
      "example: Suppose we have butadiene (as the diene) and ethene (as the dienophile). When they undergo a Diels-Alder reaction, they combine to form cyclohexene.\n",
      "\n",
      "notes: - It occurs through a concerted mechanism, meaning bonds are broken and formed simultaneously.\n",
      "- No catalysts or additional reagents are typically required, making it very useful in synthesis.\n",
      "- The Diels-Alder reaction follows the Alder Rule, where the orientation and position of substituents influence the main product, leading to specific stereochemistry.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structured Outputs (GPT-4o, 4o-mini)\n",
    "# ensures that the completion is always a valid JSON object with a schema that you define\n",
    "# Enable by passing a dict \"response_format\" = {\"type\": \"json_schema\", \"json_schema\": (your schema)}. In this schema,\n",
    "# - additionalProperties must be false, strict must be true, and required must contain all properties\n",
    "# - the types string, number, integer, boolean, object, array, null, and enum are supported\n",
    "# - union types are supported, and are represented as arrays of types; in particular, you can make a param 'optional' by making its type [\"string\", \"null\"]\n",
    "# - references are supported, and can point to either subschemas ({\"$ref\": \"#/$defs/property\"} or recurse ({\"$ref\": \"#\"})\n",
    "\n",
    "# Example: Schema for answering a question\n",
    "if demo:\n",
    "    import json\n",
    "    answer_schema = {\n",
    "        \"name\": \"answer_question\",\n",
    "        \"description\": \"Answers a question\",\n",
    "        \"strict\": True,\n",
    "        \"schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"thoughts\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"description\": \"This is optional scratch paper the assistant may use to privately draft, construct, or verify an answer.\"\n",
    "                },\n",
    "                \"explanation\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"description\": \"An optional description and explanation of the answer for the user.\"\n",
    "                },\n",
    "                \"answer\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The concise final answer, sans explanation, preambling, etc.\"\n",
    "                },\n",
    "                \"example\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"description\": \"An optional example to be provided, if appropriate.\"\n",
    "                },\n",
    "                \"notes\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"description\": \"Optional additional information or notes for the user.\"\n",
    "                }\n",
    "            },\n",
    "            \"additionalProperties\": False,\n",
    "            \"required\": [\n",
    "                \"thoughts\", \"explanation\", \"answer\", \"example\", \"notes\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    # the json object will be returned as a string (serialized), so json.loads should be used to turn it into a schema-compatible dictionary\n",
    "    # streaming is incompatible with the Structured Outputs feature, so it might take a bit for the finished answer to appear\n",
    "    answer_creator = lambda q: json.loads(chat_completion(q, {\"model\": \"gpt-4o\", \"response_format\": {\"type\": \"json_schema\", \"json_schema\": answer_schema}}))\n",
    "    my_question = \"What is a Diels-Alder reaction? Please explain like I only know the basics of organic chemistry.\"\n",
    "\n",
    "    for k, v in answer_creator(my_question).items():\n",
    "        print(f\"{k}: {v}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
