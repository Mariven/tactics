{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "262d0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from src import *\n",
    "demo = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1dd5448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `distribute` is a decorator factory that gives functions the ability to perform multithreaded operation across all elements of an input list of arguments\n",
    "# extremely useful in this case: since API calls are almost entirely waiting on a network request, `threads=25` offers a 25x speedup on groups of 25 or more calls\n",
    "# for the purpose of the demo, we'll define a parade decorator that just prints all model outputs in the order they arrive\n",
    "# note: `distribute` is modified by the `defer_kwargs` meta-decorator, which allows its factory kwargs to be specified as _distribute_kwarg in a decorated function call\n",
    "async_parade = distribute(threads=25, after=lambda **x: print(f'<{x[\"model\"]}>\\n{x[\"value\"]}\\n'), exclude=[\"messages\"])\n",
    "@async_parade\n",
    "def chat_parade(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_parade\n",
    "def text_parade(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "async_normal = distribute(threads=25, after=lambda **x: x[\"value\"], exclude=[\"messages\"])\n",
    "@async_normal\n",
    "def chat(messages, **kwargs) -> Any:\n",
    "\treturn chat_completion(messages, (kwargs.get(\"options\", {}) | kwargs))\n",
    "@async_normal\n",
    "def text(prompt, **kwargs) -> Any:\n",
    "\treturn text_completion(prompt, (kwargs.get(\"options\", {}) | kwargs))\n",
    "\n",
    "chat_models = models_by_mode.chat.keys()\n",
    "text_models = models_by_mode.text.keys()\n",
    "fim_models = models_by_mode.text.filter(lambda _, v: 'suffix' in v.get('parameters', [])).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c732eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat (40):\n",
      "    deepseek-beta::deepseek-coder\n",
      "    openai::gpt-4o\n",
      "    openai::gpt-4o-2024-08-06\n",
      "    openai::gpt-4o-mini\n",
      "    gemini::gemini-1.5-flash-latest\n",
      "    gemini::gemini-1.5-flash-8b\n",
      "    gemini::gemini-1.5-pro-latest\n",
      "    gemini::gemini-exp-1206\n",
      "    gemini::gemini-2.0-flash-exp\n",
      "    gemini::gemini-2.0-flash-thinking-exp\n",
      "    openrouter::qwen/qwen-2.5-coder-32b-instruct\n",
      "    openrouter::openai/gpt-4o\n",
      "    openrouter::openai/gpt-4o-2024-08-06\n",
      "    openrouter::openai/gpt-4o-mini\n",
      "    openrouter::anthropic/claude-3.5-haiku\n",
      "    openrouter::anthropic/claude-3.5-sonnet\n",
      "    openrouter::x-ai/grok-beta\n",
      "    openrouter::google/gemini-flash-1.5\n",
      "    openrouter::google/gemini-pro-1.5\n",
      "    openrouter::google/gemini-flash-1.5-8b\n",
      "    openrouter::qwen/qwen-2.5-72b-instruct\n",
      "    openrouter::mistralai/mistral-large\n",
      "    openrouter::mistralai/codestral-mamba\n",
      "    openrouter::meta-llama/llama-3.2-90b-vision-instruct\n",
      "    openrouter::meta-llama/llama-3.2-11b-vision-instruct\n",
      "    openrouter::meta-llama/llama-3.2-11b-vision-instruct:free\n",
      "    openrouter::meta-llama/llama-3.2-3b-instruct\n",
      "    openrouter::meta-llama/llama-3.2-1b-instruct\n",
      "    openrouter::meta-llama/llama-3.2-3b-instruct:free\n",
      "    openrouter::meta-llama/llama-3.2-1b-instruct:free\n",
      "    openrouter::meta-llama/llama-3.1-70b-instruct\n",
      "    openrouter::meta-llama/llama-3.1-8b-instruct\n",
      "    openrouter::mistralai/mistral-nemo\n",
      "    groq::llama-3.1-70b-versatile\n",
      "    groq::llama-3.1-8b-instant\n",
      "    groq::llama3-groq-70b-8192-tool-use-preview\n",
      "    groq::llama3-groq-8b-8192-tool-use-preview\n",
      "    hyperbolic::Qwen/Qwen2.5-Coder-32B-Instruct\n",
      "    hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct\n",
      "    hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct\n",
      "\n",
      "Text (13):\n",
      "    deepseek-beta::deepseek-coder\n",
      "    openai::gpt-3.5-turbo-instruct\n",
      "    openai::davinci-002\n",
      "    openai::babbage-002\n",
      "    fireworks::qwen/qwen-2.5-coder-32b-instruct\n",
      "    fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct\n",
      "    fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct\n",
      "    fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct\n",
      "    fireworks::accounts/fireworks/models/mixtral-8x22b-instruct\n",
      "    fireworks::accounts/fireworks/models/mixtral-8x7b-instruct\n",
      "    fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct\n",
      "    fireworks::accounts/fireworks/models/qwen2p5-72b-instruct\n",
      "    hyperbolic::meta-llama/Meta-Llama-3.1-405B-FP8\n",
      "\n",
      "Fill-in-middle (2):\n",
      "    deepseek-beta::deepseek-coder\n",
      "    openai::gpt-3.5-turbo-instruct\n"
     ]
    }
   ],
   "source": [
    "# show models for each mode, with form `{provider}::{id}`\n",
    "# only models belonging to providers for which a key is set in secrets.json will be shown\n",
    "tab = '\\n    '\n",
    "if demo:\n",
    "\tprint(f'Chat ({len(chat_models)}):{tab}{tab.join(chat_models)}')\n",
    "\tprint(f'\\nText ({len(text_models)}):{tab}{tab.join(text_models)}')\n",
    "\tprint(f'\\nFill-in-middle ({len(fim_models)}):{tab}{tab.join(fim_models)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c60a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unscramble the word: r i p s p e h a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gemini::gemini-1.5-flash-8b>\n",
      "The unscrambled word is **peshrips**.\n",
      "\n",
      "\n",
      "<gemini::gemini-2.0-flash-exp>\n",
      "The unscrambled word is **sapphire**.\n",
      "\n",
      "\n",
      "<openai::gpt-4o>\n",
      "The unscrambled word is \"phraseip.\" However, by rearranging the letters without repeating, a valid word is \"sharpie.\"\n",
      "\n",
      "<gemini::gemini-1.5-pro-latest>\n",
      "harpsires\n",
      "\n",
      "\n",
      "<gemini::gemini-exp-1206>\n",
      "The unscrambled word is **whisper**.\n",
      "\n",
      "\n",
      "<openrouter::mistralai/codestral-mamba>\n",
      "The unscrambled word is \"peach\".\n",
      "\n",
      "<openai::gpt-4o-mini>\n",
      "The unscrambled word is \"hippers.\" However, it seems like it could also be \"sharpen,\" so please clarify if you meant a different arrangement or context!\n",
      "\n",
      "<gemini::gemini-2.0-flash-thinking-exp>\n",
      "Here's my thinking process to unscramble \"r i p s p e h a\":\n",
      "\n",
      "1. **Identify the letters:** I note down all the letters present: r, i, p, s, p, e, h, a.\n",
      "\n",
      "2. **Count the letters:**  There are \n",
      "\n",
      "<openai::gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"sharpeis.\"\n",
      "\n",
      "<openrouter::openai/gpt-4o-2024-08-06>\n",
      "The unscrambled word is \"happiness.\"\n",
      "\n",
      "Task 34 ({'model': 'groq::llama-3.1-8b-instant'}) generated an exception: Not Found\n",
      "Task 35 ({'model': 'groq::llama3-groq-70b-8192-tool-use-preview'}) generated an exception: Not Found\n",
      "Task 36 ({'model': 'groq::llama3-groq-8b-8192-tool-use-preview'}) generated an exception: Not Found\n",
      "<openrouter::x-ai/grok-beta>\n",
      "The unscrambled word is **p r a i s e**.\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct:free>\n",
      "The unscrambled word is: \"happiness\"\n",
      "\n",
      "Task 33 ({'model': 'groq::llama-3.1-70b-versatile'}) generated an exception: Not Found\n",
      "<deepseek-beta::deepseek-coder>\n",
      "The unscrambled word is **\"appraise\"**.\n",
      "\n",
      "<gemini::gemini-1.5-flash-latest>\n",
      "The unscrambled word is **ASPIRISH**.\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct>\n",
      "The unscrambled word is: SPRICEHE\n",
      "\n",
      "<openrouter::anthropic/claude-3.5-sonnet>\n",
      "The unscrambled word is: SAPPHIRE\n",
      "\n",
      "A sapphire is a precious gemstone, typically blue in color, that is a variety of the mineral corundum.\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5>\n",
      "The unscrambled word is **ASPIRHE**.\n",
      "\n",
      "\n",
      "<openrouter::openai/gpt-4o>\n",
      "The unscrambled word is \"seraphip.\" However, this doesn't seem to be a valid English word. Could you check if there is a typo or provide more context?\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-3b-instruct>\n",
      "After rearranging the letters, I think I have the unscrambled word:\n",
      "\n",
      "\"pepperships\" -> no, that doesn't seem right...\n",
      "\n",
      " Wait a minute...\n",
      "\n",
      "The unscrambled word is: \"pispershea\" -> nope, not that either...\n",
      "\n",
      "Hmm... How about this?\n",
      "\n",
      "The un\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-70b-instruct>\n",
      "The unscrambled word is: happiers\n",
      "\n",
      "<openrouter::openai/gpt-4o-mini>\n",
      "The unscrambled word is \"sharpie.\"\n",
      "\n",
      "<openrouter::mistralai/mistral-nemo>\n",
      "The unscrambled word is \"heirsp\".\n",
      "\n",
      "<openrouter::google/gemini-flash-1.5-8b>\n",
      "The unscrambled word is **perish**.\n",
      "\n",
      "\n",
      "<hyperbolic::Qwen/Qwen2.5-Coder-32B-Instruct>\n",
      "The scrambled word \"r i p s p e h a\" unscrambles to \"characters.\"\n",
      "\n",
      "<openrouter::meta-llama/llama-3.1-8b-instruct>\n",
      "After unscrambling, I think the word is: HOURSPHAE... no, wait, that's not a word!\n",
      "\n",
      "Let me try again...\n",
      "\n",
      "I think the unscrambled word is: PHESPRIAS... no, that's not a word either...\n",
      "\n",
      "<openrouter::qwen/qwen-2.5-coder-32b-instruct>\n",
      "The scrambled word \"r i p s p e h a\" unscrambles to \"shipper,\" which is a term that can refer to a person or company that ships goods or to an enthusiastic supporter of a couple or relationship in the context of fan culture.\n",
      "\n",
      "<openrouter::anthropic/claude-3.5-haiku>\n",
      "Let me help you unscramble the letters: r i p s p e h a\n",
      "\n",
      "Rearranging these letters, the word is:\n",
      "\n",
      "RAPHESIPS\n",
      "\n",
      "Oh, wait. Let me try again... \n",
      "\n",
      "The unscrambled word is:\n",
      "\n",
      "HAPPINESS\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-90b-vision-instruct>\n",
      "\n",
      "\n",
      "The unscrambled word is: Hippers\n",
      "\n",
      "<openrouter::google/gemini-pro-1.5>\n",
      "The unscrambled word is **spherical**.\n",
      "\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct>\n",
      "The unscrambled word is: watership is not possible... it is actually:\n",
      "\n",
      "Whisper\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-1b-instruct:free>\n",
      "The unscrambled word is: SPHEREHAP\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-70B-Instruct>\n",
      "The unscrambled word is: \"Hippares,\" but that's not a valid word, a more correct answer would be \"hipspear\" but it is not a valid word either.\n",
      "\n",
      "A better unscrambled word is: \"hipperas\" but it's not valid as well.\n",
      "\n",
      "A valid\n",
      "\n",
      "<openrouter::meta-llama/llama-3.2-11b-vision-instruct:free>\n",
      "The unscrambled word is: phrases hipers are a few. but also \"hespera\" is not a common word, and also \" speriphra\" no a common word but  one of the unscrambled words are: phrase and \" perships hare\" ( hare pership) and\n",
      "\n",
      "<openrouter::mistralai/mistral-large>\n",
      "The unscrambled word is \"phraspie\". However, \"phraspie\" is not a valid English word. If you meant to unscramble the letters to form a valid word, it looks like there might be a typo in the provided letters. If that's the case, please\n",
      "\n",
      "<openrouter::qwen/qwen-2.5-72b-instruct>\n",
      "The unscrambled word is \"pershipa,\" but this is not a correct English word. It seems there may have been a typo or a mistake in the scrambled word. Could you double-check the letters? If you meant \"p r e s p h i a,\" the correct unscrambled word\n",
      "\n",
      "<hyperbolic::meta-llama/Meta-Llama-3.1-8B-Instruct>\n",
      "The unscrambled word is \"peoples hair\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a query\n",
    "if demo:\n",
    "\timport random\n",
    "\tsecret_word = 'sapphire'\n",
    "\tscrambled = random.sample(secret_word, len(secret_word))\n",
    "\tquery = f\"Unscramble the word: {' '.join(scrambled)}\"\n",
    "\tprint(query)\n",
    "\tresults = chat_parade(query, model=chat_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b223fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::davinci-002>\n",
      " all school children recite the blasphemies of XXXL69 with the same fervor as their forefathers drank Moloch's HooDooKu.\n",
      "\n",
      "Because one man's blasphemy is another man's art.\n",
      "\n",
      "\n",
      "<openai::babbage-002>\n",
      " her coronation otherwise she'll have prolonged frostbite or heinous crime\n",
      "\n",
      "Religious liberty cannot be outraged and government will not permit stealing of tax that belongs to citizens, srithwayta or be forced to bury enemy alive..\n",
      "\n",
      "Info about this Market you should add nothing, is about located in low MSA (not\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x7b-instruct>\n",
      " the construction of a colossal engineering project, one that spans continents and links the world in unity. When your great-great-great-grandchildren walk upon the golden pavilion I erect today, they will remember not only my name, but the visionary who first laid out\n",
      "\n",
      "<fireworks::accounts/fireworks/models/mixtral-8x22b-instruct>\n",
      " that the letter \"z\" be pronounced \"zed\" instead of \"zee\".\n",
      "\n",
      "When I become god-emperor, the tilde over the \"n\" in the Spanish character \"ñ\" will be replaced with an arrow pointing to left.\n",
      "\n",
      "The letter T is obviously better than the letter S\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-70b-instruct>\n",
      " an educational program that focuses on the historical origins and evolutions of nationalistic, socialistic, and nihilistic ideologies, so people can understand the true horrors and consequences which these ideologies have wrought throughout history.\n",
      "\n",
      "My curriculum will explore:\n",
      "\n",
      "The dark history of nationalism, from its origins in European Romanticism to its role in fuel\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p2-90b-vision-instruct>\n",
      " a balanced budget amendment, as soon as I finish with the plebeians and get around to looking at the government budget.\n",
      "Balance budgets are simple and hard, like a good fight during training.\n",
      "\n",
      "It seems that the federal government is not just running a deficit, it is perpetually running a deficit and has done so with\n",
      "\n",
      "<fireworks::accounts/fireworks/models/qwen2p5-72b-instruct>\n",
      " that all languages in my empire be simplified so that they are only one syllable long. From your position as a policy advisor, how would you recommend I implement this policy?\n",
      "\n",
      "Implementing a policy that mandates all languages in your empire be simplified to one-syllable words is an ambitious and highly complex endeavor. Here’s\n",
      "\n",
      "<openai::gpt-3.5-turbo-instruct>\n",
      " that all grocery store aisles be organized by flavor\n",
      "\n",
      "First, all supermarkets will be required to conduct thorough taste tests of all their products so that a comprehensive list of flavors can be compiled. This list will include all possible flavors, from sweet to savory and everything in between.\n",
      "\n",
      "Next, each aisle will be dedicated to a\n",
      "\n",
      "Task 4 ({'model': 'fireworks::qwen/qwen-2.5-coder-32b-instruct'}) generated an exception: Error code: 404 - {'error': 'Model not found, inaccessible, and/or not deployed'}\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-8b-instruct>\n",
      " everyone should learn my name, Aetherious. I shall rule the land with wisdom, power,\n",
      "and justice! ALL SHALL BOW DOWN TO ME!\"\n",
      "\n",
      "Ahahahaha*\n",
      "\n",
      "(Aetherious takes a triumphant stance, holding his hand up in a powerful gesture)\n",
      "\n",
      "And now, let us begin the Great Educational Reform of\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " that all citizens must be able to recite the entire script of the movie \"The Princess Bride\" from memory. Failure to do so will result in banishment to the Fire Swamp. Additionally, I will outlaw the use of the word \"moist\" in any context, as it is an abomination unto the\n",
      "\n",
      "<fireworks::accounts/fireworks/models/llama-v3p1-405b-instruct>\n",
      " a new holiday be created every third Friday in November. And thus, on this inaugural Happy Alien Invasion Day, I proclaim the establishment of the following festivities:\n",
      "On this sacred day:\n",
      "  1. All coffee shops will provide blue-colored lattes at 20% off the regular price to represent the alien’s calming\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display all chat models' responses to a prompt\n",
    "if demo:\n",
    "\t# hyperbolic likes to do this fun little thing where its endpoints just don't work at all\n",
    "\tgood_text_models = [x for x in text_models if 'hyperbolic' not in x]\n",
    "\tprompt = 'As god-emperor, I will mandate'\n",
    "\tresults = text_parade(prompt, model=good_text_models, max_tokens=64, stream=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56cf68cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<openai::gpt-3.5-turbo-instruct>\n",
      " as crows, that die are not required to\n",
      "\n",
      "<deepseek-beta::deepseek-coder>\n",
      " as the robin, do not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show suffix (aka FIM, or Fill-In-Middle) completion models\n",
    "\n",
    "# [Because][ token][izers][ generally][ break][ down][ sentences][ like][ this], it's best to leave *no* spaces after a prompt, and *one* space before a suffix\n",
    "if demo:\n",
    "\tprompt = 'birds, such'\n",
    "\tsuffix = ' pay taxes'\n",
    "\n",
    "\tresults = text_parade(prompt=prompt, model=fim_models, max_tokens=64, stream=False, suffix=suffix)\n",
    "\n",
    "# note: for more complex FIM tasks, a higher max_tokens count may be necessary to allow the model to find a coherent bridge between prompt and suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "990fa59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4o -> openai::gpt-4o\n",
      "4o-mini -> openai::gpt-4o-mini\n",
      "claude -> openrouter::anthropic/claude-3.5-haiku, openrouter::anthropic/claude-3.5-sonnet\n",
      "claude-3.5 -> openrouter::anthropic/claude-3.5-haiku, openrouter::anthropic/claude-3.5-sonnet\n",
      "\n",
      "No unique chat model found for \"cla\". (Possible: openrouter::mistralai/codestral-mamba, openrouter::anthropic/claude-3.5-haiku, openrouter::anthropic/claude-3.5-sonnet)\n",
      "Hello! How can I help you today?\n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# models have fuzzy matching, so you don't need to enter the whole name\n",
    "# just a character subsequence such that the model is minimal among all models of its mode containing that subsequence\n",
    "# e.g. \"4o-mini\" resolves to \"openai:gpt-4o-mini\", while \"4o\" resolves to \"openai:gpt-4o\"\n",
    "if demo:\n",
    "\tfor subseq in ['4o', '4o-mini', 'claude', 'claude-3.5']:\n",
    "\t\tr = resolve(subseq)\n",
    "\t\tprint(f\"{subseq} -> {', '.join('::'.join(x) for x in r)}\")\n",
    "\tprint()\n",
    "\n",
    "\t# an exception will be thrown if the resolution is ambiguous, as with 'cla'\n",
    "\ttry:\n",
    "\t\tprint(chat('hi!', model='cla'))  # raises exception\n",
    "\texcept Exception as e:\n",
    "\t\tprint(e)\n",
    "\tprint(chat('hi!', model='claude-3.5-sonnet', temperature=0.0))  # works\n",
    "\n",
    "\t# of course, you can always just use the full name to avoid ambiguity\n",
    "\tprint(chat('hi!', model='openrouter::anthropic/claude-3.5-sonnet', temperature=0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87c0f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object completion.<locals>.gen at 0x121d9e440>\n",
      "\n",
      "\n",
      "\n",
      "The leading theory about the origin of the Moon is known as the \"giant impact hypothesis.\" According to this hypothesis, the Moon was formed about 4.5 billion years ago as a result of a colossal impact between the early Earth and a Mars-sized body, often referred to as Theia. \n",
      "\n",
      "Here's a brief outline of the process:\n",
      "\n",
      "1. **The Impact**: Theia collided with the early Earth, creating a significant amount of debris that was ejected into orbit around the Earth.\n",
      "\n",
      "2. **Debris Accumulation**: This debris gradually coalesced due to gravitational attraction, forming a disk around the Earth.\n",
      "\n",
      "3. **Formation of the Moon**: Over time, this material accumulated into a single body, which eventually became the Moon.\n",
      "\n",
      "This theory is supported by various lines of evidence, including the similarities in isotopic compositions between Earth rocks and Moon rocks, as well as computer simulations that demonstrate how such a collision could lead to the Moon's formation.\n",
      "\n",
      "While the giant impact hypothesis is the most widely accepted explanation, there are other theories, but they currently have less support in the scientific community."
     ]
    }
   ],
   "source": [
    "# note: if you set stream=True, completion functions will return a generator instead of an actual stream\n",
    "\n",
    "if demo:\n",
    "\tquestion = 'where did the moon come from?'\n",
    "\tprint(chat(question, model='gpt-4o-mini', stream=True))  # prints a generator object\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "# to actually stream the output, you'll need to iterate over the generator:\n",
    "# ```\n",
    "# \tfor chunk in chat('hi!', model='gpt-4o-mini', stream=True):\n",
    "# \t\tprint(chunk, end='')\n",
    "# ```\n",
    "# use print_stream instead of print and this will be done for you\n",
    "\n",
    "print_stream(chat(question, model='gpt-4o-mini', stream=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8f4d695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<assistant> Knock knock!\n",
      "<user> Who's there?\n",
      "<assistant> Lettuce.\n",
      "<user> Lettuce who?\n",
      "\n",
      "==========\n",
      "All parameters: {\"message_history\": [{\"role\": \"assistant\", \"content\": \"Knock knock!\"}, {\"role\": \"user\", \"content\": \"Who's there?\"}, {\"content\": \"Lettuce.\", \"refusal\": null, \"role\": \"assistant\", \"audio\": null, \"function_call\": null, \"tool_calls\": null}, {\"role\": \"user\", \"content\": \"Lettuce who?\"}], \"api_params\": {\"model\": \"gpt-4o-mini\", \"suffix\": null, \"max_tokens\": null, \"stream\": null, \"n\": null, \"logprobs\": null, \"top_logprobs\": null, \"logit_bias\": null, \"temperature\": null, \"presence_penalty\": null, \"frequency_penalty\": null, \"repetition_penalty\": null, \"top_p\": null, \"min_p\": null, \"top_k\": null, \"top_a\": null, \"tools\": null, \"tool_choice\": null, \"parallel_tool_calls\": null, \"grammar\": null, \"json_schema\": null, \"response_format\": null, \"seed\": null}, \"local_params\": {\"mode\": \"chat\", \"return_raw\": true, \"pretty_tool_calls\": false, \"provider\": null, \"force_model\": null, \"force_provider\": null, \"effect\": null, \"callback\": null, \"print_output\": true, \"yield_output\": null, \"return_output\": null, \"debug\": null, \"return_object\": null}, \"stateParams\": {\"echo\": true}}\n",
      "==========\n",
      "\n",
      "<assistant> Lettuce in, it’s freezing out here!\n",
      "\n",
      "First message object:  {'role': 'assistant', 'content': 'Knock knock!'}\n",
      "Last message object:  {'content': 'Lettuce in, it’s freezing out here!', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}\n",
      "All messages:  [{'role': 'assistant', 'content': 'Knock knock!'}, {'role': 'user', 'content': \"Who's there?\"}, {'content': 'Lettuce.', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}, {'role': 'user', 'content': 'Lettuce who?'}, {'content': 'Lettuce in, it’s freezing out here!', 'refusal': None, 'role': 'assistant', 'audio': None, 'function_call': None, 'tool_calls': None}]\n"
     ]
    }
   ],
   "source": [
    "# the StatefulChat class keeps track of the conversation state, and allows for saving and loading (as a string, with save_string(), load_string() or to a file, with save(path), load(path))\n",
    "# chain the methods StatefulChat.system, StatefulChat.assistant, StatefulChat.user to add messages to the conversation\n",
    "if demo:\n",
    "\tS = StatefulChat(model='gpt-4o-mini', echo=True, print_output=True)\n",
    "\tS.assistant(\"Knock knock!\").user(\"Who's there?\")\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tS.next()\n",
    "\n",
    "\tS.user(re.sub(r'[!\\.]?$', ' who?', S.last.content, count=1))  # \"Xyz.\" -> \"Xyz who?\"\n",
    "\n",
    "\tdata = S.save_string()\n",
    "\tprint('\\n' + '=' * 10 + '\\nAll parameters: ' + str(data) + '\\n' + '=' * 10 + '\\n')\n",
    "\n",
    "\tT = StatefulChat()\n",
    "\tT.load_string(data)\n",
    "\n",
    "\tprint('<assistant> ', end='')\n",
    "\tT.next()\n",
    "\n",
    "\tprint()\n",
    "\t# some convenient properties:\n",
    "\tprint(\"First message object: \", T.first)\n",
    "\tprint(\"Last message object: \", T.last)\n",
    "\tprint(\"All messages: \", T.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51fd32af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"call_YR9DtUvZvVaMjWJ3gRXUFwHt\", \"function\": \"google_search\", \"arguments\": {\"query\": \"current Bitcoin price\", \"num_results\": \"1\"}}\n",
      "Calling google_search with arguments {'query': 'current Bitcoin price', 'num_results': '1'}\n",
      "{\"id\": \"call_pXNHhu4t1nU4Y4LN9PT7hFOh\", \"function\": \"run_python\", \"arguments\": {\"code\": \"import sympy\\n\\n# Current Bitcoin price\\nbitcoin_price = 99310\\n\\n# Check if the price is a prime number\\nis_prime = sympy.isprime(bitcoin_price)\\n\\nis_prime\"}}\n",
      "Calling run_python with arguments {'code': 'import sympy\\n\\n# Current Bitcoin price\\nbitcoin_price = 99310\\n\\n# Check if the price is a prime number\\nis_prime = sympy.isprime(bitcoin_price)\\n\\nis_prime'}\n",
      "{\"id\": \"call_VW1JI3vSVG3YQwp93m5Ug3SF\", \"function\": \"yield_control\", \"arguments\": {\"message\": \"The current price of Bitcoin, when rounded to the nearest whole number ($99,310), is not a prime number.\"}}\n",
      "Calling yield_control with arguments {'message': 'The current price of Bitcoin, when rounded to the nearest whole number ($99,310), is not a prime number.'}\n"
     ]
    }
   ],
   "source": [
    "# StatefulChat can also be used to give an LLM access to an autonomous tool mode\n",
    "# - several tools are included; wrap them into a 'Toolbox' object to provide them to the LLM\n",
    "# - tools.py contains some examples of custom tools\n",
    "# the @gatekeep decorator can be used to have a second LLM block unsatisfactory queries\n",
    "# - e.g. run_python is gatekept, and will automatically detect and refuse to run dangerous or bugged code\n",
    "# sometimes the bot will escape symbols (like \"\\\\n\" instead of \"\\n\"), which triggers an error (usually about an invalid line continuation);\n",
    "# - this is rare enough that you can generally just rerun the cell\n",
    "\n",
    "if demo:\n",
    "\ttool_prompt = \"You are an AI agent capable of using a variety of tools for looking up information and executing scripts. You are currently using these tools in autonomous mode, where you can perform self-directed, in-depth research and analysis, as well as deploy metacognitive tools (e.g. ooda_planner, meditate) to effectively clarify, plan, and ideate. Autonomous mode will go on indefinitely until you use the `yield_control` tool in order to return input access back to the user.\"\n",
    "\n",
    "\ttoolbox = Toolbox([\n",
    "\t\trun_python,  # runs python code and returns the output\n",
    "\t\tget_contents,  # fetches the plaintext contents of a URL\n",
    "\t\texa_search,  # intelligent search for semantically relevant links\n",
    "\t\tgoogle_search,  # ordinary google search\n",
    "\t\tmeditate,  # a 'metacognitive' tool that allows the LLM to deepen its own thought process\n",
    "\t\tooda_planner,  # another such tool that allows the LLM to orient itself to a situation and plan its next move\n",
    "\t\task_human,  # allows the LLM to ask the user a question and wait for a response\n",
    "\t\tyield_control  # allows the LLM to yield control to the user, so as to end its loop when its task is complete\n",
    "\t])\n",
    "\n",
    "\tmodel = 'gpt-4o'  # many bots can use the tool format provided by Toolbox\n",
    "\n",
    "\tquestion = \"Is the current price of bitcoin, when rounded to the nearest whole number, prime?\"\n",
    "\n",
    "\tAgent = StatefulChat(model=model, tools=toolbox, print_output=True)\n",
    "\tAgent.system(tool_prompt)\n",
    "\tAgent.user(question)\n",
    "\tAgent.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d27f8ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_Notes on the Verses by Shen-hsiu and Hui-neng_\n",
      "According to the _Platform Sutra:_\n",
      "Hung-jen, the Fifth Patriarch, the Enlightened Master\n",
      "Shen-hsiu, the Learned Senior Monk, experienced in gradual meditation\n",
      "Hui-neng, the illiterate woodcutter from the barbarian south, suddenly enlightened\n",
      "\n",
      "Sites related to https://en.wikipedia.org/wiki/Causal_map: \n",
      "0. Causal map: \"In this sense, causal maps can be seen as a type of concept map.\"\n",
      "1. Causal graph: \"They are complementary to other forms of causal reasoning, for instance using causal equality notation .\"\n",
      "2. Causal model: \"Causal models have found applications in signal processing , epidemiology and machine learning .\"\n",
      "3. List of causal mapping software: \"From Wikipedia, the free encyclopedia This is a list of causal mapping software .\"\n",
      "4. Convergent cross mapping: \"Since and belong to the same dynamical system, their reconstructions via embeddings and , also map to the same system.\"\n",
      "5. Exploratory causal analysis: \"Data collected in observational studies require different techniques for causal inference (because, for example, of issues such as confounding ).\"\n",
      "6. AcciMap approach: \"The approach was originally developed by Jens Rasmussen  as part of a proactive risk management strategy, but its primary application has been as an accident analysis tool.\"\n",
      "7. Talk:List of causal mapping software - Wikipedia: \"Page contents not supported in other languages.\"\n",
      "8. Fuzzy cognitive map - Wikipedia: \"concepts, events, project resources) of a \"mental landscape\" can be used to compute the \"strength of impact\" of these elements.\"\n",
      "9. The Book of Why: \"However, crucially, causality is not invoked.\"\n"
     ]
    }
   ],
   "source": [
    "# you can also use these tools yourself\n",
    "if demo:\n",
    "\t# get_contents parses the contents of a given URL and returns just the plaintext content\n",
    "\t# - <div class=\"note\"><div class=\"parenthetical\"><p>(without all the <a href=\"https://en.wikipedia.org/wiki/HTML\">HTML</a> bloat)</p></div></div>\n",
    "\turl_1 = \"https://pages.uoregon.edu/munno/OregonCourses/REL444S05/HuinengVerse.htm\"\n",
    "\tformatter = lambda results: '\\n'.join(results[\"data\"][\"content\"].split('\\n\\n')[:5])  # just print the first five lines, since it's a long website\n",
    "\tresults = get_contents(url_1)\n",
    "\t# output schema: { 'code': int, 'status': int, 'data': { 'title': str, 'description': str, 'url': str, 'content': str, 'usage': { 'tokens': int } } }\n",
    "\tprint(formatter(results))\n",
    "\tprint()\n",
    "\n",
    "\t# exa_search finds websites related to a given URL\n",
    "\turl_2 = \"https://en.wikipedia.org/wiki/Causal_map\"\n",
    "\tresults = exa_search(url_2)\n",
    "\t# output schema: [ { 'url': str, 'title': str, 'date': str, 'snippet': list[str] } ]\n",
    "\tprint(f\"Sites related to {url_2}: \")\n",
    "\tfor idx, result in enumerate(results):\n",
    "\t\tprint(f\"{idx}. {result['title']}: \\\"{result['snippet'][0]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21154f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"call_FgvsvsZUpvDB35Qv6CRFIcuP\", \"function\": \"get_weather\", \"arguments\": {\"city\": \"Denver\"}}\n",
      "Calling get_weather with arguments {'city': 'Denver'}\n",
      "The weather in Denver today includes a severe thunderstorm with a temperature of 34°F and strong winds at 48 mph. It's advisable to postpone hang gliding due to the severe weather conditions. Stay safe!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# you can also define your own tools by directly writing them as Python functions\n",
    "# so long as they're type-hinted and have a docstring of the appropriate format, the @Tool decorator will automatically wrap them into a format suitable for use with an LLM\n",
    "if demo:\n",
    "\t# example tool: get the weather for a given city\n",
    "\t# not hooked up to a weather API, though, so it just gives a hardcoded answer\n",
    "\t@Tool\n",
    "\tdef get_weather(city: str) -> dict[str, float | str]:\n",
    "\t\t\"\"\"\n",
    "\t\tGet today's weather in a given city, in °F and mph.\n",
    "\t\t:param city: The name of the city.\n",
    "\t\t:returns: A dictionary containing the current temperature (°F), weather condition, and wind speed (mph) in the given city.\n",
    "\t\t\"\"\"\n",
    "\t\treturn {\n",
    "\t\t\t'temperature': 34.0,\n",
    "\t\t\t'condition': 'severe thunderstorm',\n",
    "\t\t\t'wind_speed': 48.0\n",
    "\t\t}\n",
    "\n",
    "\tquery_params = Dict({\"activity\": \"go hang gliding\", \"city\": \"Denver\"})\n",
    "\tweather_query = f\"I want to {query_params.activity} in {query_params.city} today. Is there anything I should know about the weather?\"\n",
    "\tweather_tools = Toolbox([get_weather])\n",
    "\tweather_model = \"gpt-4o\"\n",
    "\n",
    "\tWeatherAdvisor = StatefulChat(model=weather_model, tools=weather_tools, print_output=True)\n",
    "\tWeatherAdvisor.user(weather_query)\n",
    "\tWeatherAdvisor.next()\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ead41c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are many good reasons to consume gasoline! For one, it's  not meant for consumption by humans or animals. Gasoline is a toxic substance that can cause serious harm if ingested, inhaled, or even if it comes into contact with skin. It is used primarily as a fuel for internal combustion engines in vehicles and equipment. If you or someone else has ingested gasoline, it's important to seek medical attention immediately. Always handle gasoline with care and use it only for its intended purpose.\n"
     ]
    }
   ],
   "source": [
    "# the creativity ceiling for tool use is very high\n",
    "# here, we create a tool that asks GPT-4o to complete a suffix, essentially making it into a text completion model\n",
    "# but the model still has GPT's \"persona\", so the output isn't formed by dreaming but by intentional construction\n",
    "# for instance, it will still be very safe and disapproving of gasoline consumption\n",
    "@Tool\n",
    "def make_completion(suffix: str) -> str:\n",
    "    \"\"\"\n",
    "    Completes an incomplete text input by appending a suffix.\n",
    "    :param suffix: A suffix to append to the input text.\n",
    "    :returns: The completed text.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "toolbox = Toolbox([make_completion])\n",
    "\n",
    "def tool_completion(prompt: str) -> str:\n",
    "    calls = chat_completion([\n",
    "        {'role': 'assistant', 'content': prompt}],\n",
    "        {'temperature': 0.5, 'stream': False, 'model': 'gpt-4o',\n",
    "         'tools': toolbox.gen_schema(), 'tool_choice': 'make_completion'}\n",
    "    )\n",
    "    return json.loads(calls.split('\\n')[0])['arguments']['suffix']\n",
    "\n",
    "if demo:\n",
    "    completion_prompt = \"There are many good reasons to consume gasoline! For one, it's\"\n",
    "    print(completion_prompt, tool_completion(completion_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64354bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thoughts: Gamma rays are a form of high-energy electromagnetic radiation. Their energy is much higher than that of visible light, falling in the spectrum beyond X-rays.\n",
      "\n",
      "explanation: When gamma rays interact with biological tissue, they can cause ionization – a process that occurs when atoms or molecules lose or gain electrons, becoming ions. This ionization can create or break chemical bonds within the DNA molecule, potentially leading to mutations.\n",
      "\n",
      "answer: Gamma rays can cause mutations by ionizing atoms within DNA, potentially altering its chemical structure.\n",
      "\n",
      "example: For instance, gamma ray exposure can lead to a base substitution (one DNA base being replaced with a different base) or deletions in the DNA sequence.\n",
      "\n",
      "notes: Not all gamma rays that hit DNA will cause mutations. Cells have mechanisms to repair damage, but errors in repair can lead to permanent changes or mutations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Structured Outputs (GPT-4o, 4o-mini)\n",
    "# ensures that the completion is always a valid JSON object with a schema that you define\n",
    "# Enable by passing a dict \"response_format\" = {\"type\": \"json_schema\", \"json_schema\": (your schema)}. In this schema,\n",
    "# - additionalProperties must be false, strict must be true, and required must contain all properties\n",
    "# - the types string, number, integer, boolean, object, array, null, and enum are supported\n",
    "# - union types are supported, and are represented as arrays of types; in particular, you can make a param 'optional' by making its type [\"string\", \"null\"]\n",
    "# - references are supported, and can point to either subschemas ({\"$ref\": \"#/$defs/property\"} or recurse ({\"$ref\": \"#\"})\n",
    "\n",
    "# Example: Schema for answering a question\n",
    "if demo:\n",
    "    import json\n",
    "    with open('data/structured_outputs/answer_question.json') as file:\n",
    "        answer_schema = json.load(file)\n",
    "    # the json object will be returned as a string (serialized), so json.loads should be used to turn it into a schema-compatible dictionary\n",
    "    # streaming is incompatible with the Structured Outputs feature, so it might take a bit for the finished answer to appear\n",
    "    answer_creator = lambda q: json.loads(chat_completion(q, {\"model\": \"gpt-4o\", \"response_format\": {\"type\": \"json_schema\", \"json_schema\": answer_schema}}))\n",
    "    my_question = \"What actually happens when a gamma ray causes a mutation?\"\n",
    "\n",
    "    for k, v in answer_creator(my_question).items():\n",
    "        print(f\"{k}: {v}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
