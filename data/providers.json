{
	"providers": [
		{
			"id": "deepseek-beta",
			"api": {
				"url": "https://api.deepseek.com/beta",
				"key-secret": "deepseek-beta"
			},
			"models": [
				{
					"id": "deepseek-coder",
					"mode": "text",
					"capacity": {
						"input": 128000,
						"output": 8000
					},
					"cost": {
						"input": 0.14,
						"output": 0.28,
						"cached": 0.014
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","suffix","response_format"],
					"short": "DeepSeek 2.5 (DS Beta)"
				},
				{
					"id": "deepseek-coder",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 8000
					},
					"cost": {
						"input": 0.14,
						"output": 0.28,
						"cached": 0.014
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"]
				}
			]
		},
		{
			"id": "openai",
			"api": {
				"url": "https://api.openai.com/v1",
				"key-secret": "openai"
			},
			"models": [
				{
					"id": "gpt-4o",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 4096
					},
					"cost": {
						"input": 5,
						"output": 15,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools","tool_choice","parallel_tool_calls","logit_bias","logprobs","top_logprobs","presence_penalty","frequency_penalty","n","stream"],
					"short": "GPT-4o (OAI)"
				},
				{
					"id": "gpt-4o-2024-08-06",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 16384
					},
					"cost": {
						"input": 2.5,
						"output": 10,
						"cached": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools","tool_choice","parallel_tool_calls","logit_bias","logprobs","top_logprobs","presence_penalty","frequency_penalty","n","stream"]
				},
				{
					"id": "gpt-4o-mini",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 16384
					},
					"cost": {
						"input": 0.15,
						"output": 0.6,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools","tool_choice","parallel_tool_calls","logit_bias","logprobs","top_logprobs","presence_penalty","frequency_penalty","n","stream"],
					"short": "GPT-4o Mini (OAI)"
				},
				{
					"id": "gpt-3.5-turbo-instruct",
					"mode": "text",
					"capacity": {
						"input": 4095,
						"output": 4096
					},
					"cost": {
						"input": 1.5,
						"output": 2,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","n","suffix","stream"],
					"short": "GPT-3.5 Turbo Inst (OAI)"
				},
				{
					"id": "davinci-002",
					"mode": "text",
					"capacity": {
						"input": 16835,
						"output": 4096
					},
					"cost": {
						"input": 2,
						"output": 2,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","n","stream"],
					"short": "davinci-002 (OAI)"
				},
				{
					"id": "babbage-002",
					"mode": "text",
					"capacity": {
						"input": 4095,
						"output": 4096
					},
					"cost": {
						"input": 0.4,
						"output": 0.4,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","n","stream"],
					"short": "babbage-002 (OAI)"
				}
			]
		},
		{
			"id": "openrouter",
			"api": {
				"url": "https://openrouter.ai/api/v1",
				"key-secret": "openrouter"
			},
			"models": [
				{
					"id": "openai/gpt-4o",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 16384
					},
					"cost": {
						"input": 5,
						"output": 15,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "GPT-4o (OAI)"
				},
				{
					"id": "openai/gpt-4o-2024-08-06",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 16384
					},
					"cost": {
						"input": 2.5,
						"output": 10,
						"cached": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"]
				},
				{
					"id": "openai/gpt-4o-mini",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 16384
					},
					"cost": {
						"input": 0.15,
						"output": 0.6,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "GPT-4o Mini (OAI)"
				},
				{
					"id": "openai/o1-preview",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 32768
					},
					"cost": {
						"input": 15,
						"output": 60,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "GPT-4o1 Preview (OR)",
					"disabled": true
				},
				{
					"id": "anthropic/claude-3.5-sonnet",
					"mode": "chat",
					"capacity": {
						"input": 200000,
						"output": 8192
					},
					"cost": {
						"input": 3,
						"output": 15,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Claude 3.5 Sonnet (OR)"
				},
				{
					"id": "meta-llama/llama-3.1-405b-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 131072
					},
					"cost": {
						"input": 1.79,
						"output": 1.79,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.1 405B Inst (OR)",
					"disabled": true
				},
				{
					"id": "google/gemini-flash-1.5",
					"mode": "chat",
					"capacity": {
						"input": 4000000,
						"output": 32768
					},
					"cost": {
						"input": 0.0375,
						"output": 0.15,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Gemini Flash 1.5 (OR)"
				},
				{
					"id": "google/gemini-pro-1.5",
					"mode": "chat",
					"capacity": {
						"input": 4000000,
						"output": 32768
					},
					"cost": {
						"input": 2.5,
						"output": 7.5,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Gemini Pro 1.5 (OR)"
				},
				{
					"id": "google/gemini-flash-1.5-8b",
					"mode": "chat",
					"capacity": {
						"input": 1000000,
						"output": 8192
					},
					"cost": {
						"input": 0.0375,
						"output": 0.15,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Gemini Flash 1.5 8B (OR)"
				},
				{
					"id": "qwen/qwen-2.5-72b-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 32000
					},
					"cost": {
						"input": 0.35,
						"output": 0.40,
						"cached": 0.00
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools","tool_choice","frequency_penalty","presence_penalty","repetition_penalty","top_k"],
					"short": "Qwen 2.5 72B Inst (OR)"
				},
				{
					"id": "mistralai/mistral-large",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 128000
					},
					"cost": {
						"input": 2,
						"output": 6,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Mistral Large 2 (OR)"
				},
				{
					"id": "mistralai/codestral-mamba",
					"mode": "chat",
					"capacity": {
						"input": 256000,
						"output": 256000
					},
					"cost": {
						"input": 0.25,
						"output": 0.25,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Codestral Mamba (OR)"
				},
				{
					"id": "nousresearch/hermes-3-llama-3.1-405b",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 18000
					},
					"cost": {
						"input": 4.5,
						"output": 4.5,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","repetition_penalty","top_k","min_p","top_a"],
					"short": "Nous Hermes 3 Llama 3.1 405B (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-90b-vision-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 131072
					},
					"cost": {
						"input": 0.9,
						"output": 0.9,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2V 90B Inst (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-11b-vision-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 131072
					},
					"cost": {
						"input": 0.2,
						"output": 0.2,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2V 11B Inst (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-11b-vision-instruct:free",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 4096
					},
					"cost": {
						"input": 0.0,
						"output": 0.0,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2V 11B Inst Free (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-3b-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 4096
					},
					"cost": {
						"input": 0.05,
						"output": 0.03,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2 3B Inst (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-1b-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 4096
					},
					"cost": {
						"input": 0.02,
						"output": 0.01,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2 1B Inst (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-3b-instruct:free",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 4096
					},
					"cost": {
						"input": 0.00,
						"output": 0.00,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2 3B Inst Free (OR)"
				},
				{
					"id": "meta-llama/llama-3.2-1b-instruct:free",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 4096
					},
					"cost": {
						"input": 0.00,
						"output": 0.00,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Llama 3.2 1B Inst Free (OR)"
				},
				{
					"id": "meta-llama/llama-3.1-70b-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 131072
					},
					"cost": {
						"input": 0.3,
						"output": 0.3,
						"cached": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"]
				},
				{
					"id": "meta-llama/llama-3.1-8b-instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 131072
					},
					"cost": {
						"input": 0.2,
						"output": 0.2,
						"cached": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"]
				},
				{
					"id": "mistralai/mistral-nemo",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 32768
					},
					"cost": {
						"input": 0.13,
						"output": 0.13,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","response_format","tools"],
					"short": "Mistral Nemo (OR)"
				}
			]
		},
		{
			"id": "fireworks",
			"api": {
				"url": "https://api.fireworks.ai/inference/v1",
				"key-secret": "fireworks"
			},
			"models": [
				{
					"id": "accounts/fireworks/models/llama-v3p1-405b-instruct",
					"mode": "text",
					"capacity": {
						"input": 131072,
						"output": 16384
					},
					"cost": {
						"input": 1.79,
						"output": 1.79,
						"cached": 0.0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","top_k"],
					"short": "Llama 3.1 405B Inst (FW)"
				},
				{
					"id": "accounts/fireworks/models/llama-v3p1-70b-instruct",
					"mode": "text",
					"capacity": {
						"input": 131072,
						"output": 16384
					},
					"cost": {
						"input": 0.3,
						"out": 0.3,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","top_k"],
					"short": "Llama 3.1 70B Inst (FW)"
				},
				{
					"id": "accounts/fireworks/models/llama-v3p1-8b-instruct",
					"mode": "text",
					"capacity": {
						"input": 131072,
						"output": 16384
					},
					"cost": {
						"input": 0.055,
						"out": 0.055,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","top_k"],
					"short": "Llama 3.1 8B Inst (FW)"
				},
				{
					"id": "accounts/fireworks/models/mixtral-8x22b-instruct",
					"mode": "text",
					"capacity": {
						"input": 65536,
						"output": 8192
					},
					"cost": {
						"input": 0.65,
						"out": 0.65,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","top_k"],
					"short": "Mixtral 8x22B Inst (FW)"
				},
				{
					"id": "accounts/fireworks/models/mixtral-8x7b-instruct",
					"mode": "text",
					"capacity": {
						"input": 32768,
						"output": 4096
					},
					"cost": {
						"input": 0.24,
						"out": 0.24,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","top_k"],
					"short": "Mixtral 8x7B Inst (FW)"
				},
				{
					"id": "accounts/fireworks/models/llama-v3p2-90b-vision-instruct",
					"mode": "text",
					"capacity": {
						"input": 131072,
						"output": 131072
					},
					"cost": {
						"input": 0.9,
						"out": 0.9,
						"cached": 0
					}
				},
				{
					"id": "accounts/fireworks/models/qwen2p5-72b-instruct",
					"mode": "text",
					"capacity": {
						"input": 32768,
						"output": 32768
					},
					"cost": {
						"input": 0.9,
						"out": 0.9,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop","presence_penalty","frequency_penalty","top_k"],
					"short": "Mixtral 8x7B Inst (FW)"
				}
			]
		},
		{
			"id": "groq",
			"api": {
				"url": "https://api.groq.com/openai/v1",
				"key-secret": "groq"
			},
			"models": [
				{
					"id": "llama-3.1-70b-versatile",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 8000
					},
					"cost": {
						"input": 0.0,
						"output": 0.0,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop"],
					"short": "Llama 3.1 70B (Groq)"
				},
				{
					"id": "llama-3.1-8b-instant",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 8000
					},
					"cost": {
						"input": 0.0,
						"output": 0.0,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop"],
					"short": "Llama 3.1 8B (Groq)"
				},
				{
					"id": "llama3-groq-70b-8192-tool-use-preview",
					"mode": "chat",
					"capacity": {
						"input": 8192,
						"output": 4096
					},
					"cost": {
						"input": 0,
						"output": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","tools"]
				},
				{
					"id": "llama3-groq-8b-8192-tool-use-preview",
					"mode": "chat",
					"capacity": {
						"input": 8192,
						"output": 4096
					},
					"cost": {
						"input": 0,
						"output": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop","tools"]
				}
			]
		},
		{
			"id": "hyperbolic",
			"api": {
				"url": "https://api.hyperbolic.xyz/v1",
				"key-secret": "hyperbolic"
			},
			"models": [
				{
					"id": "meta-llama/Llama-3.2-90B-Vision",
					"mode": "text",
					"capacity": {
						"input": 4096,
						"output": 4096
					},
					"cost": {
						"input": 0.4,
						"output": 0.4,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop"],
					"short": "Llama 3.2 90B Base (Hyp FP16)"
				},
				{
					"id": "meta-llama/Meta-Llama-3.1-405B-Instruct",
					"mode": "chat",
					"capacity": {
						"input": 131072,
						"output": 32768
					},
					"cost": {
						"input": 4,
						"output": 4,
						"cached": 0.0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop"],
					"short": "Llama 3.1 405B Inst (Hyp FP16)"
				},
				{
					"id": "meta-llama/Meta-Llama-3.1-70B-Instruct",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 8192
					},
					"cost": {
						"input": 0,
						"output": 0,
						"cached": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop"]
				},
				{
					"id": "meta-llama/Meta-Llama-3.1-8B-Instruct",
					"mode": "chat",
					"capacity": {
						"input": 128000,
						"output": 4096
					},
					"cost": {
						"input": 0,
						"output": 0,
						"cached": 0
					},
					"parameters": ["messages","temperature","top_p","max_tokens","stop"]
				},
				{
					"id": "meta-llama/Meta-Llama-3.1-405B-FP8",
					"mode": "text",
					"capacity": {
						"input": 32768,
						"output": 32768
					},
					"cost": {
						"input": 4,
						"out": 4,
						"cached": 0
					},
					"parameters": ["prompt","temperature","top_p","max_tokens","stop"],
					"short": "Llama 3.1 405B Base (Hyp FP16)"
				}
			]
		}
	]
}